{
  "format": "wiki_dump_json",
  "format_version": 1,
  "generated_at": "2025-12-20T22:16",
  "title": "ROOT FILES",
  "root_dir": "C:\\MyCode\\AbstractWiki\\abstract-wiki-architect\\docs",
  "stats": {
    "file_count": 14,
    "total_size_bytes": 89563,
    "total_size_mb": 0.0854
  },
  "nav": {
    "home_file": null,
    "prev_file": null,
    "next_file": "docs_20251220_2216_02_internal.json",
    "prev_title": "",
    "next_title": "internal"
  },
  "files": [
    {
      "rel_path": "01-ARCHITECTURE_GLOBAL.md",
      "ext": ".md",
      "size_bytes": 5217,
      "kind": "markdown",
      "content": "\n# Abstract Wiki Architect: Global Architecture\n\n## 1\\. System Overview\n\nAbstract Wiki Architect is a **Rule-Based Natural Language Generation (NLG) Engine**. Unlike statistical models (LLMs) that predict the next token based on probability, this system constructs sentences deterministically using linguistic rules and structured data. It is designed to generate high-quality, verifiable biographical and encyclopedic sentences across diverse language families (Indo-European, Uralic, Bantu, Altaic, etc.).\n\n[Image of Natural Language Generation Architecture Diagram]\n\n## 2\\. Core Philosophy: Separation of Concerns\n\nThe architecture is strictly layered to ensure modularity. Changes in the vocabulary (Lexicon) do not break the grammar rules (Matrix), and changes in grammar do not break the sentence templates (Renderer).\n\n### Layer A: The Lexicon (Data)\n\n  * **Role:** The \"Vocabulary.\" Stores words (lemmas) and their inherent properties (gender, stems, irregular forms, QIDs).\n  * **Strategy:** **Usage-Based Sharding**. Data is organized by domain rather than monolithic files to optimize loading and relevance.\n      * **Core:** High-frequency functional words (copulas, pronouns, articles).\n      * **People:** Biographical terms (professions, titles, relationships).\n      * **Science/Geo:** Domain-specific terminology.\n  * [cite_start]**Source:** Wikidata is the upstream \"raw material\" (referenced via Q-IDs); local JSON files are the downstream runtime optimization[cite: 38].\n\n### Layer B: The Grammar Matrix (Logic)\n\n  * **Role:** The \"Rules.\" Defines how words change form (Morphology) and how they relate to one another (Syntax).\n  * **Mechanism:** Shared configurations (Matrices) per language family avoid code duplication.\n      * [cite_start]**Romance Matrix:** Handles gender agreement, pluralization, and article selection (e.g., *le* vs *la* vs *l'*)[cite: 62, 69].\n      * [cite_start]**Germanic Matrix:** Handles strong/weak verb conjugation (e.g., *sing* $\\to$ *sang*) and shifting vowel stems[cite: 706].\n      * [cite_start]**Slavic Matrix:** Handles complex case declension (Nominative vs. Instrumental) required for predicates (e.g., Polish: *jest fizykiem*)[cite: 795].\n      * [cite_start]**Agglutinative Matrix:** Handles vowel harmony (Front vs. Back vowels) and suffix stacking for languages like Turkish and Hungarian[cite: 692].\n      * [cite_start]**Japonic/Polysynthetic:** Handles particle attachment (e.g., *wa*, *no*) and deep suffixation logic[cite: 118, 802].\n\n### Layer C: The Renderer (Presentation)\n\n  * **Role:** The \"Assembly.\" Takes an abstract intent (Data) and combines it with the Lexicon and Grammar Matrix to produce a final string.\n  * **Templates:** Logic-agnostic patterns such as `{name} {copula} {profession} {nationality}.`\n  * **Output Example:**\n      * *Input:* `Marie Curie | Physicist | Polish`\n      * *French Output:* \"Marie Curie est une physicienne polonaise.\"\n      * *German Output:* \"Marie Curie ist eine polnische Physikerin.\"\n\n## 3\\. Data Flow\n\nThe generation pipeline follows a strict four-step process:\n\n1.  **Input (Abstract Intent):**\n    The system receives a semantic frame (JSON) describing *who* and *what* to describe.\n\n    ```json\n    { \"name\": \"Shaka\", \"profession\": \"warrior\", \"nationality\": \"zulu\" }\n    ```\n\n2.  **Lookup (Lexicon Layer):**\n    The loader fetches the relevant terms from `data/lexicon/{lang}/{domain}.json`.\n\n      * *Fetch:* \"warrior\" $\\to$ Target Lemma\n      * *Fetch:* \"zulu\" $\\to$ Target Adjective\n\n3.  **Inflection (Morphology Engine):**\n    The engine applies rules from `data/morphology_configs/`.\n\n      * *Check:* Does the profession need to agree with the subject's gender?\n      * *Check:* Does the adjective require a specific case or prefix?\n      * *Apply:* Modify the lemma (e.g., add suffix `-a`, change vowel `o` $\\to$ `u`).\n\n4.  **Realization (Renderer):**\n    The inflected words are slotted into the language-specific sentence template.\n\n      * *Finalize:* Capitalization, punctuation, and phonological smoothing (e.g., elision).\n\n## 4\\. Directory Structure Map\n\n| Directory | Purpose | Examples |\n| :--- | :--- | :--- |\n| **`data/lexicon/`** | Stores words split by language and domain. | `en/core.json`, `fr/people.json` |\n| **`data/morphology_configs/`** | Logic for language families. | `romance_grammar_matrix.json`, `slavic_matrix.json` |\n| **`data/romance/`, `data/germanic/`** | Language-specific connector configs. | `fr.json`, `de.json`, `es.json` |\n| **`data/raw_wikidata/`** | Staging area for raw data dumps. | `*.json.gz` (Ignored by git) |\n\n## 5\\. Supported Language Families\n\n  * **Indo-European:** Germanic (En, De, Nl, Sv...), Romance (Fr, Es, It, Pt, Ro), Slavic (Ru, Pl, Cs), Celtic (Cy), Indo-Aryan (Hi, Bn), Iranic (Fa).\n  * **Altaic / Agglutinative:** Turkic (Tr), Japonic (Ja), Koreanic (Ko).\n  * **Uralic:** Finno-Ugric (Fi, Hu).\n  * **Austronesian:** Malayo-Polynesian (Id).\n  * **Bantu:** Swahili (Sw).\n  * **Inuit-Yupik-Unangan:** Inuktitut (Iu).\n  * **Dravidian:** Tamil (Ta), Malayalam (Ml).\n  * **Semitic:** Arabic (Ar), Hebrew (He).\n  * **Sinitic:** Mandarin (Zh).\n\n"
    },
    {
      "rel_path": "02-LEXICON_ARCHITECTURE.md",
      "ext": ".md",
      "size_bytes": 5160,
      "kind": "markdown",
      "content": "# Lexicon Architecture & Workflow\n\n## 1\\. Strategy: Source vs. Usage\n\nWe distinguish between where data comes from (Source) and how it is organized for the application (Usage).\n\n  * **Upstream Source:** **Wikidata**.\n\n      * We treat Wikidata as the raw material. [cite_start]All lexical entries should ideally link back to a Wikidata QID (e.g., `Q142` for France) to maintain verifiable data lineage[cite: 302, 536, 557].\n      * [cite_start]Raw dumps or temporary processing files belong in `data/raw_wikidata/`, which is ignored by version control[cite: 805].\n\n  * **Downstream Usage:** **Domain-Sharded JSON**.\n\n      * Instead of monolithic files per language, we organize data by **semantic domain** (e.g., Science, People, Geography).\n      * [cite_start]This allows the system to load only what is necessary, improves maintainability, and simplifies debugging[cite: 461, 805].\n\n## 2\\. Directory Structure\n\nThe legacy flat structure (e.g., `data/lexicon/en_lexicon.json`) is deprecated. The new standard uses a nested, language-specific folder structure.\n\n```text\ndata/lexicon/\n‚îú‚îÄ‚îÄ schema.json          # Master validation schema (Draft-07)\n‚îú‚îÄ‚îÄ loader.py            # Runtime script: Merges domain files into one dict\n‚îú‚îÄ‚îÄ en/                  # English Namespace\n‚îÇ   ‚îú‚îÄ‚îÄ core.json        # High-quality manual entries: verbs 'to be', pronouns\n‚îÇ   ‚îú‚îÄ‚îÄ people.json      # Biography: Professions, titles, family relations\n‚îÇ   ‚îú‚îÄ‚îÄ science.json     # Domain: Scientific fields, awards, terminology\n‚îÇ   ‚îî‚îÄ‚îÄ geography.json   # Domain: Countries, cities, demonyms\n‚îú‚îÄ‚îÄ fr/                  # French Namespace\n‚îÇ   ‚îú‚îÄ‚îÄ core.json\n‚îÇ   ‚îú‚îÄ‚îÄ people.json\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îî‚îÄ‚îÄ ...\n```\n\n[cite_start]This structure is supported by `data/lexicon/init.py`, which allows the directory to be treated as a package if needed[cite: 461, 462].\n\n## 3\\. Domain Definitions\n\nWe organize files based on the *topic* of the content to facilitate modular loading.\n\n  * **`core.json`**: The grammatical \"skeleton\" of the language.\n\n      * [cite_start]*Contents:* Copulas (to be), auxiliary verbs, articles, pronouns, and basic high-frequency nouns (person, man, woman)[cite: 254, 267].\n      * *Maintenance:* High - likely manual curation.\n\n  * **`people.json`**: Entities and predicates for biographical generation.\n\n      * [cite_start]*Contents:* Professions (Physicist, Writer), Titles (King, President), relationships, and human-centric verbs (born, died)[cite: 294, 296, 304].\n      * *Source:* Often semi-automated imports from Wikidata.\n\n  * **`science.json`**: Specialized vocabulary for scientific biographies.\n\n      * [cite_start]*Contents:* Fields of study (Physics, Chemistry), awards (Nobel Prize), and specific verbs (discover, publish)[cite: 316, 320, 326].\n\n  * **`geography.json`**: Location data.\n\n      * [cite_start]*Contents:* Countries, cities, and their associated adjectival/demonym forms (e.g., \"France\" $\\to$ \"French\")[cite: 298, 299].\n\n## 4\\. The Schema (Simplified)\n\n[cite_start]Every entry in the lexicon files must adhere to the schema defined in `data/lexicon_schema.json`[cite: 38].\n\n### Base Entry Structure\n\n```json\n\"physicist\": {\n  [cite_start]\"pos\": \"NOUN\",            // Part of Speech: NOUN, VERB, ADJ [cite: 46]\n  [cite_start]\"gender\": \"m\",            // Grammatical Gender (m, f, n, common) [cite: 48]\n  [cite_start]\"human\": true,            // Semantic tag for \"Who\" vs \"What\" [cite: 48]\n  [cite_start]\"qid\": \"Q169470\",         // Link to Wikidata [cite: 51]\n  [cite_start]\"forms\": {                // Explicit overrides for irregulars [cite: 44]\n    \"pl\": \"physicists\"\n  }\n}\n```\n\n### Key Schema Types\n\n  * [cite_start]**Professions:** Inherit from Base Entry but are specifically tagged for professional roles[cite: 52].\n  * [cite_start]**Nationalities:** Include specific fields for `adjective` (e.g., \"Polish\"), `demonym` (noun for the person), and `country_name`[cite: 53, 54].\n  * [cite_start]**Titles:** Include positioning logic (e.g., `pre_name` for \"Dr.\", `post_name` for \"PhD\")[cite: 56, 57].\n\n## 5\\. Workflow: Adding a New Word\n\n### Step 1: Identify Domain\n\n  * Is it a functional word like \"is\" or \"the\"? $\\to$ `core.json`\n  * Is it a job title like \"Senator\"? $\\to$ `people.json`\n  * Is it a chemical element or scientific theory? $\\to$ `science.json`\n\n### Step 2: Extract from Wikidata\n\nUse the QID to find the target language label.\n\n  * [cite_start]*Example:* `Q169470` $\\to$ \"physicien\" (French)[cite: 391].\n\n### Step 3: Define Morphology\n\nAdd necessary properties based on the language family's Grammar Matrix.\n\n  * [cite_start]**Romance:** Define `gender` (m/f)[cite: 772].\n  * [cite_start]**Slavic:** Define stem or case endings if irregular[cite: 791].\n  * [cite_start]**Agglutinative:** Ensure vowel harmony rules can apply (e.g., front/back vowels)[cite: 692].\n\n### Step 4: Validate\n\nRun the validation script to ensure the JSON adheres to `lexicon_schema.json` and that the word inflects correctly in a generated test sentence.\n\n"
    },
    {
      "rel_path": "03-API_REFERENCE.md",
      "ext": ".md",
      "size_bytes": 5896,
      "kind": "markdown",
      "content": "# API Reference & Semantic Frames\n\n## 1\\. Overview\n\nThe Abstract Wiki Architect API accepts a **Semantic Frame** (a JSON object representing meaning) and returns natural language text in the requested target language. The system serves as a deterministic NLG engine, converting abstract intents into readable sentences based on the linguistic rules defined in the backend architecture.\n\n**Base URL:** `POST /api/v1/generate`\n\n## 2\\. Request Format\n\n  * **Method:** `POST`\n  * **Header:** `Content-Type: application/json`\n  * **Query Parameter:** `lang` (Required, e.g., `en`, `fr`, `tr`, `sw`). [cite_start]The target language code must match a configuration in `data/morphology_configs/` or `data/romance/`, etc. [cite: 692, 706, 772]\n  * **Body:** A single Semantic Frame object.\n\n### Generic Request Example\n\n```bash\ncurl -X POST \"http://localhost:8000/api/v1/generate?lang=fr\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n           \"frame_type\": \"bio\",\n           \"name\": \"Marie Curie\",\n           \"profession\": \"physicist\",\n           \"nationality\": \"polish\"\n         }'\n```\n\n## 3\\. Semantic Frame Schemas\n\nThe system logic (Renderer) dispatches processing based on the `frame_type` field. Below are the supported schemas.\n\n### A. Bio Frame (`bio`)\n\nUsed for introductory biographical sentences. This frame triggers the `BiographicalRenderer` logic, which handles profession agreement, nationality inflection, and article selection.\n\n| Field | Type | Required | Description |\n| :--- | :--- | :--- | :--- |\n| `frame_type` | String | Yes | Must be `\"bio\"`. |\n| `name` | String | Yes | The subject's name (e.g., \"Alan Turing\"). |\n| `profession` | String | Yes | [cite_start]Key identifying a profession in the `people.json` lexicon (e.g., \"computer\\_scientist\"). [cite: 313, 447, 494] |\n| `nationality` | String | No | [cite_start]Key identifying a nationality in the `geography.json` lexicon (e.g., \"british\"). [cite: 265, 456, 499] |\n| `gender` | String | No | `\"m\"`, `\"f\"`, or `\"n\"`. [cite_start]Used for morphological agreement if the lexicon entry is ambiguous or missing. [cite: 275, 414, 587] |\n\n**Example:**\n\n```json\n{\n  \"frame_type\": \"bio\",\n  \"name\": \"Grace Hopper\",\n  \"profession\": \"computer_scientist\",\n  \"nationality\": \"american\",\n  \"gender\": \"f\"\n}\n```\n\n### B. Relational Frame (`relational`)\n\nUsed to express a direct relationship between two entities. This is useful for describing family ties, academic advisors, or political successors.\n\n| Field | Type | Required | Description |\n| :--- | :--- | :--- | :--- |\n| `frame_type` | String | Yes | Must be `\"relational\"`. |\n| `subject` | String | Yes | The agent/subject of the sentence. |\n| `relation` | String | Yes | The predicate key (e.g., \"spouse\\_of\", \"advisor\\_to\", \"child\\_of\"). |\n| `object` | String | Yes | The patient/object or target of the relation. |\n\n**Example:**\n\n```json\n{\n  \"frame_type\": \"relational\",\n  \"subject\": \"Pierre Curie\",\n  \"relation\": \"spouse_of\",\n  \"object\": \"Marie Curie\"\n}\n```\n\n### C. Event Frame (`event`)\n\nUsed for temporal events like birth, death, awards, or discoveries.\n\n| Field | Type | Required | Description |\n| :--- | :--- | :--- | :--- |\n| `frame_type` | String | Yes | Must be `\"event\"`. |\n| `event_type` | String | Yes | The type of event: `\"birth\"`, `\"death\"`, `\"award\"`, `\"discovery\"`. |\n| `subject` | String | Yes | The entity experiencing the event. |\n| `date` | String | No | ISO date or year (e.g., \"1934\"). |\n| `location` | String | No | [cite_start]City or Country key found in the lexicon (e.g., \"Paris\", \"Germany\"). [cite: 301, 599] |\n\n**Example:**\n\n```json\n{\n  \"frame_type\": \"event\",\n  \"event_type\": \"birth\",\n  \"subject\": \"Albert Einstein\",\n  \"date\": \"1879\",\n  \"location\": \"germany\"\n}\n```\n\n## 4\\. Response Format\n\n### Success (200 OK)\n\nThe API returns a JSON object containing the generated text and metadata about the generation process.\n\n```json\n{\n  \"result\": \"Grace Hopper est une informaticienne am√©ricaine.\",\n  \"meta\": {\n    \"engine\": \"RomanceRenderer\",\n    \"lang\": \"fr\",\n    \"latency_ms\": 12\n  }\n}\n```\n\n### Error Handling\n\n| Status Code | Error Type | Description |\n| :--- | :--- | :--- |\n| **400** | Bad Request | Missing `frame_type` or required fields in the request body. |\n| **404** | Not Found | [cite_start]The requested `lang` is not supported in the configuration (e.g., `morphology_configs` is missing). [cite: 62, 706] |\n| **422** | Unprocessable Entity | A specific lexicon entry (profession, nationality) could not be found in the loaded dictionary files (e.g., \"spaceman\" not in `people.json`). |\n| **500** | Internal Server Error | An unexpected failure in the morphology engine (e.g., missing matrix configuration for a requested language family). |\n\n## 5\\. Integration Patterns\n\n### Frontend Integration\n\nWhen building a frontend for this API, it is recommended to:\n\n1.  **Validate Input:** Ensure `frame_type` matches one of the supported schemas before sending.\n2.  **Handle Fallbacks:** If the API returns a 422 (missing word), the frontend should fallback to displaying the raw data or a generic placeholder (e.g., \"Grace Hopper (computer\\_scientist)\").\n3.  **Language Selection:** Query the supported languages endpoint (if available) or hardcode the list based on the `data/` directory structure.\n\n### Python Client Example\n\n```python\nimport requests\n\ndef generate_bio(name, profession, nationality, lang=\"en\"):\n    url = \"http://localhost:8000/api/v1/generate\"\n    payload = {\n        \"frame_type\": \"bio\",\n        \"name\": name,\n        \"profession\": profession,\n        \"nationality\": nationality\n    }\n    response = requests.post(url, params={\"lang\": lang}, json=payload)\n    return response.json()\n\n# Usage\nprint(generate_bio(\"Marie Curie\", \"physicist\", \"polish\", lang=\"fr\"))\n"
    },
    {
      "rel_path": "04-DEPLOYMENT.md",
      "ext": ".md",
      "size_bytes": 3105,
      "kind": "markdown",
      "content": "# Deployment Guide\n\n## 1. Overview\nAbstract Wiki Architect is composed of two primary services:\n* **Backend:** Python/FastAPI (Port 8000)\n* **Frontend:** Next.js (Port 3000) - *Optional UI*\n\n## 2. Local Development (Bare Metal)\n\n### Backend\n```bash\n# Install dependencies\npip install -r requirements.txt\n\n# Run the API server with hot-reload\nuvicorn architect_http_api.main:app --reload --port 8000 --host 0.0.0.0\n````\n\n### Frontend (Optional)\n\n```bash\ncd abstractwiki-frontend\nnpm install\nnpm run dev\n```\n\n## 3\\. Docker Deployment\n\nWe provide Dockerfiles for containerized deployment.\n\n### Backend Container\n\n```bash\n# Build\ndocker build -f docker/Dockerfile.backend -t abstractwiki-backend .\n\n# Run\ndocker run -d --name abstractwiki-backend \\\n  -p 8000:8000 \\\n  -v $(pwd)/data:/app/data \\\n  abstractwiki-backend\n```\n\n*Note: We mount the `./data` volume so you can update the lexicon without rebuilding the image.*\n\n### Frontend Container\n\n```bash\n# Build\ndocker build -f docker/Dockerfile.frontend -t abstractwiki-frontend .\n\n# Run\ndocker run -d --name abstractwiki-frontend \\\n  -p 3000:3000 \\\n  --link abstractwiki-backend \\\n  -e NEXT_PUBLIC_API_URL=http://abstractwiki-backend:8000 \\\n  abstractwiki-frontend\n```\n\n## 4\\. Hybrid Environment: Linux (WSL) & Windows\n\nThis setup is ideal for developers who prefer coding on Windows (VS Code) but running the backend in a native Linux environment.\n\n### The Problem\n\n  * **Line Endings:** Windows uses `CRLF`, Linux uses `LF`. Python scripts with `CRLF` can crash in Linux.\n  * **Paths:** Windows paths (`C:\\`) confuse Linux tools.\n\n### The Solution: WSL 2\n\nUse **Windows Subsystem for Linux (WSL 2)** to run the backend while keeping your IDE in Windows.\n\n#### Step 1: Install WSL\n\nOpen PowerShell as Administrator and run:\n\n```powershell\nwsl --install\n```\n\nRestart your computer if prompted.\n\n#### Step 2: Set Up the Project in WSL\n\n1.  Open your WSL terminal (Ubuntu).\n2.  Clone the repository **inside the Linux file system** (e.g., `~/projects/`), NOT on the mounted Windows drive (`/mnt/c/`).\n      * *Why?* Faster file I/O and avoids permission issues.\n    <!-- end list -->\n    ```bash\n    cd ~\n    mkdir projects\n    cd projects\n    git clone <your-repo-url>\n    ```\n\n#### Step 3: Connect VS Code\n\n1.  Install the **\"WSL\" extension** in VS Code.\n2.  In your WSL terminal, navigate to the project folder and type:\n    ```bash\n    code .\n    ```\n    This opens VS Code on Windows, but it \"talks\" directly to the Linux file system.\n\n#### Step 4: Line Ending Hygiene\n\nTo prevent `CRLF` issues, create a `.gitattributes` file in the root directory:\n\n```text\n* text=auto eol=lf\n```\n\nThis forces Git to checkout files with Linux line endings (`LF`), even on Windows.\n\n## 5\\. Environment Variables\n\n| Variable | Description | Default |\n| :--- | :--- | :--- |\n| `ARCHITECT_ENV` | `dev` or `prod` | `dev` |\n| `LOG_LEVEL` | `DEBUG`, `INFO`, `WARNING` | `INFO` |\n| `WIKIDATA_CACHE_DIR` | Path to store raw dumps | `./data/raw_wikidata` |\n\n"
    },
    {
      "rel_path": "05-ADDING_A_LANGUAGE.md",
      "ext": ".md",
      "size_bytes": 5054,
      "kind": "markdown",
      "content": "# Developer Guide: Adding a New Language\n\nThis guide documents the standard workflow for adding support for a new language (e.g., `pt` for Portuguese or `ko` for Korean) to the Abstract Wiki Architect.\n\n## Prerequisite Checklist\n\n1.  **ISO Code:** Identify the 2-letter ISO 639-1 code (e.g., `pt`, `ko`, `fi`).\n2.  **Language Family:** Determine if the language fits an existing family matrix (Romance, Germanic, Slavic, Agglutinative) or requires a new one.\n\n-----\n\n## Step 1: Create the Lexicon Structure\n\nThe lexicon is no longer a single file. You must create a namespace folder for your language.\n\n**Action:** Create directory `data/lexicon/{lang_code}/`.\n\n**Required Files:**\nAt a minimum, you must create the following JSON files in that directory:\n\n### 1\\. `core.json`\n\nContains essential grammar function words.\n\n```json\n{\n  \"verb_be\": {\n    \"pos\": \"VERB\",\n    \"lemma\": \"ser\",\n    \"forms\": { \"pres_3sg\": \"√©\", \"past_3sg\": \"foi\" }\n  },\n  \"art_indef_m\": { \"pos\": \"ART\", \"lemma\": \"um\" }\n}\n```\n\n### 2\\. `geography.json`\n\nContains the country name and nationality adjective for the language itself (to allow \"X is a Portuguese physicist\").\n\n```json\n{\n  \"portugal\": { \"pos\": \"NOUN\", \"gender\": \"m\", \"qid\": \"Q45\" },\n  \"portuguese\": { \n    \"pos\": \"ADJ\", \n    \"forms\": { \"m\": \"portugu√™s\", \"f\": \"portuguesa\" },\n    \"qid\": \"Q17413\" \n  }\n}\n```\n\n*Note: You can add `people.json` and `science.json` later as you expand vocabulary.*\n\n-----\n\n## Step 2: Configure Morphology\n\n### Scenario A: Language fits an existing Family (e.g., Portuguese $\\to$ Romance)\n\nIf the language belongs to a supported family (Romance, Germanic, Slavic, Agglutinative), you only need a configuration file.\n\n**Action:** Create `data/{family}/{lang_code}.json`.\n\n**Example (`data/romance/pt.json`):**\n\n```json\n{\n  \"meta\": { \"family\": \"romance\", \"code\": \"pt\" },\n  \"articles\": {\n    \"m\": { \"default\": \"um\" },\n    \"f\": { \"default\": \"uma\" }\n  },\n  \"morphology\": {\n    \"suffixes\": [\n      { \"ends_with\": \"or\", \"replace_with\": \"ora\" },\n      { \"ends_with\": \"o\", \"replace_with\": \"a\" }\n    ]\n  },\n  \"structure\": \"{name} √© {article} {profession} {nationality}.\"\n}\n```\n\n### Scenario B: Language requires a NEW Family Matrix\n\nIf the language structure is fundamentally different (e.g., adding Klingon or a specialized Polysynthetic language not covered by existing matrices).\n\n1.  **Create Matrix:** `data/morphology_configs/klingon_matrix.json`.\n2.  **Define Rules:** Define the specific inflection logic (e.g., prefixes, vowel harmony groups) in the matrix.\n\n-----\n\n## Step 3: Register in the Engine\n\nThe system uses a **MorphologyFactory** to instantiate the correct processor.\n\n**Location:** `architect_engine/morphology/factory.py` (or equivalent registry).\n\n### For Standard Languages (Scenario A)\n\n**No code changes are usually required.** The `MorphologyFactory` automatically scans the `data/` directories. If it finds `data/romance/pt.json`, it knows to instantiate the `RomanceMorphology` class with the Portuguese configuration.\n\n### For Custom Logic (Scenario B or Complex Cases)\n\nIf your language requires logic that JSON configuration cannot capture (e.g., complex Sandhi rules in Sanskrit or specific particle attachment order in Japanese), you must create a custom class.\n\n1.  **Create Class:**\n\n    ```python\n    # architect_engine/morphology/custom/portuguese.py\n    from ..romance import RomanceMorphology\n\n    class PortugueseMorphology(RomanceMorphology):\n        def apply_sandhi(self, word, next_word):\n            # Custom logic to handle \"de\" + \"o\" -> \"do\"\n            if word == \"de\" and next_word == \"o\":\n                return \"do\"\n            return f\"{word} {next_word}\"\n    ```\n\n2.  **Register:** Update the Factory to map `'pt'` to `PortugueseMorphology`.\n\n-----\n\n## Step 4: Validation & Testing\n\n### 1\\. Validate JSON Schemas\n\nRun the schema validator to ensure your new lexicon files match `data/lexicon_schema.json`.\n\n```bash\npython scripts/validate_lexicon.py --lang=pt\n```\n\n### 2\\. Generate a Test Sentence\n\nUse the CLI or API to generate a bio frame to verify the pipeline.\n\n```bash\n# Using the CLI tool\npython main.py generate --lang=pt --name=\"Marie Curie\" --prof=\"physicist\" --nat=\"polish\"\n```\n\n**Expected Output:**\n\n> \"Marie Curie √© uma f√≠sica polonesa.\"\n\n### 3\\. Add Regression Test\n\nAdd a test case to `tests/test_generation.py` to prevent future regressions.\n\n```python\ndef test_portuguese_generation():\n    result = generate_bio(\"Marie Curie\", \"physicist\", \"polish\", lang=\"pt\")\n    assert result == \"Marie Curie √© uma f√≠sica polonesa.\"\n```\n\n-----\n\n## Summary of Files to Add\n\n| File Path | Purpose |\n| :--- | :--- |\n| `data/lexicon/{lang}/core.json` | Basic vocabulary. |\n| `data/lexicon/{lang}/geography.json` | Country/Nationality data. |\n| `data/{family}/{lang}.json` | Grammar configuration and sentence template. |\n| `tests/test_{lang}.py` | (Optional) Specific unit tests. |"
    },
    {
      "rel_path": "06_ROADMAP.md",
      "ext": ".md",
      "size_bytes": 2485,
      "kind": "markdown",
      "content": "# 06\\. Roadmap & Future Architecture\n\n## 1\\. Overview\n\nThe current architecture (V1) relies on in-memory JSON loading and regex-based morphology. While sufficient for prototyping and low-latency demonstrations with limited vocabularies, it faces scalability bottlenecks as the lexicon grows. This roadmap outlines the transition to V2.\n\n## 2\\. Database Migration Strategy\n\n  * **Current State:** Flat JSON files loaded into Python dictionaries at startup.\n  * **Limitation:** Memory usage scales linearly with lexicon size; startup time increases; no relational integrity.\n  * **V2 Target:** **Relational/Graph Storage** (SQLite/PostgreSQL).\n      * **Schema:** `lexicon` (lemmas), `forms` (inflections), `morphology` (rules).\n      * **Benefits:** $O(1)$ indexed lookups, lazy loading, and SQL constraints for data integrity.\n\n## 3\\. Advanced Morphology Engine (FST)\n\n  * **Current State:** `MorphologyBuilder` classes using Python string manipulation (regex).\n  * **Limitation:** Difficult to handle non-concatenative morphology (Semitic roots) or polysynthetic sandhi (Inuktitut).\n  * **V2 Target:** **Finite State Transducers (FST)**.\n      * **Libraries:** Pynini or HFST.\n      * **Mechanism:** Morphology defined as a graph of states (Root $\\to$ Suffix A $\\to$ Suffix B).\n\n## 4\\. Grammatical Framework (GF) Integration\n\n*Status: Research / Experimental*\n\nWe are exploring alignment with the **Grammatical Framework (GF)** to leverage its Resource Grammar Library (RGL).\n\n  * **Strategy:** \"Parasitic\" Morphology.\n  * **Mechanism:** Map semantic frames (`Bio`, `Event`) to GF Abstract Syntax Trees.\n  * **Benefit:** \"Outsource\" complex inflection (e.g., Finnish cases, Russian aspect) to GF's RGL while keeping the lightweight Python engine for simpler languages.\n\n## 5\\. API & Generation Evolution\n\n  * **Current State:** Single-sentence generation.\n  * **V2 Target:** **Document-Level Generation**.\n      * **Discourse Planning:** Ordering facts logically (Birth $\\to$ Career $\\to$ Death).\n      * **Aggregation:** Combining sentences (\"She is X. She is Y.\" $\\to$ \"She is X and Y.\").\n      * **Referring Expressions:** Generating pronouns (\"She\", \"Her\") naturally.\n\n## 6\\. Quality Assurance Pipeline\n\n  * **Current State:** Unit tests checking string equality.\n  * **V2 Target:** **Round-Trip Evaluation**.\n      * Generate text in Language X $\\to$ Translate to English via External API $\\to$ Compare with original intent.\n\n-----\n"
    },
    {
      "rel_path": "AI_Services.md",
      "ext": ".md",
      "size_bytes": 13058,
      "kind": "markdown",
      "content": "= Abstract Wiki Architect: AI Services =\n\nThis document defines the integrated Artificial Intelligence layer of the Abstract Wiki Architect. The system consolidates all AI interactions into a unified `ai_services` package, using Large Language Models (Gemini) to handle tasks requiring semantic understanding, code repair, and linguistic quality assurance.\n\n== The Three AI Personas ==\n\nThe architecture delegates responsibilities to three distinct AI agents, each serving a specific phase of the natural language generation pipeline.\n\n{| class=\"wikitable\"\n! Agent !! Role !! Responsibility !! Implementation\n|-\n| '''The Lexicographer'''\n| Data Generator\n| Bootstrap dictionaries for new languages by generating morphological classes and seed lexicons (e.g., classifying \"Apple\" as a Noun with specific gender/plural forms).\n| <code>ai_services/lexicographer.py</code>\n|-\n| '''The Surgeon'''\n| Code Fixer\n| Implements the \"Self-Healing\" build pipeline. Reads GF compiler error logs and surgically patches broken <code>.gf</code> source files to resolve API mismatches or syntax errors.\n| <code>ai_services/surgeon.py</code>\n|-\n| '''The Judge'''\n| Quality Assurance\n| Generates \"Gold Standard\" reference sentences and validates the engine's output by scoring linguistic naturalness and accuracy against the source abstract intent.\n| <code>ai_services/judge.py</code>\n|}\n\n== Directory Structure ==\n\nAll AI logic is centralized in the <code>ai_services/</code> directory to ensure consistent API handling and rate limiting.\n\n<syntaxhighlight lang=\"text\">\nai_services/\n‚îú‚îÄ‚îÄ __init__.py           # Package exposure (exports surgeon, judge, lexicographer)\n‚îú‚îÄ‚îÄ client.py             # Central Gemini client (API keys, Config, Rate Limiting)\n‚îú‚îÄ‚îÄ lexicographer.py      # Logic for seeding dictionaries\n‚îú‚îÄ‚îÄ surgeon.py            # Logic for repairing broken grammars\n‚îî‚îÄ‚îÄ judge.py              # Logic for validation & gold standard generation\n</syntaxhighlight>\n\n== Configuration & Settings ==\n\nThe AI layer requires specific environment variables and file paths to function.\n\n* '''Environment Variable:''' <code>GOOGLE_API_KEY</code> (Required in <code>.env</code> file).\n* '''Model Selection:''' Defaults to <code>gemini-1.5-pro</code> for high-reasoning tasks (configured in <code>client.py</code>).\n* '''Failure Report Path:''' <code>data/reports/build_failures.json</code> (The shared memory file used by the Compiler to pass error logs to the Surgeon).\n\n== Core Modules ==\n\n=== 1. The Client (client.py) ===\n'''Entry Point:''' <code>ai_services/client.py</code>\n\nThis module manages the connection to the LLM provider, handling authentication, error states, and rate limiting centrally. It implements exponential backoff to handle API quotas robustly.\n\n<syntaxhighlight lang=\"python\">\nimport os\nimport time\nimport logging\nimport google.generativeai as genai\nfrom dotenv import load_dotenv\n\n# --- Configuration ---\nload_dotenv()\nAPI_KEY = os.getenv(\"GOOGLE_API_KEY\")\nMODEL_NAME = os.getenv(\"ARCHITECT_AI_MODEL\", \"gemini-1.5-pro\")\n\n# --- Logging Setup ---\nlogger = logging.getLogger(\"ai_services\")\nif not logger.handlers:\n    handler = logging.StreamHandler()\n    formatter = logging.Formatter('%(asctime)s - %(levelname)s - [AI] %(message)s')\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    logger.setLevel(logging.INFO)\n\n_model = None\n\ndef _initialize():\n    \"\"\"Initializes the Gemini client (Singleton Pattern).\"\"\"\n    global _model\n    if _model: return True\n    \n    if not API_KEY:\n        logger.error(\"Missing GOOGLE_API_KEY in environment variables.\")\n        return False\n\n    try:\n        genai.configure(api_key=API_KEY)\n        _model = genai.GenerativeModel(MODEL_NAME)\n        logger.info(f\"Connected to Google AI ({MODEL_NAME})\")\n        return True\n    except Exception as e:\n        logger.critical(f\"Connection failed: {e}\")\n        return False\n\ndef generate(prompt, max_retries=3):\n    \"\"\"\n    Robust wrapper for text generation with error handling and backoff.\n    \"\"\"\n    if not _initialize(): return \"\"\n\n    wait_time = 2  # Start with 2 seconds wait\n\n    for attempt in range(1, max_retries + 1):\n        try:\n            response = _model.generate_content(prompt)\n            if response.prompt_feedback and response.prompt_feedback.block_reason:\n                logger.warning(f\"Blocked: {response.prompt_feedback.block_reason}\")\n                return \"\"\n            return response.text.strip()\n\n        except Exception as e:\n            logger.warning(f\"Attempt {attempt}/{max_retries} failed: {e}\")\n            if attempt < max_retries:\n                time.sleep(wait_time)\n                wait_time *= 2\n            else:\n                logger.error(\"AI Generation failed after max retries.\")\n                return \"\"\n    return \"\"\n</syntaxhighlight>\n\n=== 2. The Judge (judge.py) ===\n'''Entry Point:''' <code>ai_services/judge.py</code>\n\nThe Judge is the Quality Assurance agent responsible for \"grading\" the output of the deterministic engine. It enforces strict JSON output schemas for machine readability.\n\n<syntaxhighlight lang=\"python\">\nimport json\nimport logging\nfrom . import client\n\nlogger = logging.getLogger(\"ai_services.judge\")\n\ndef _clean_json_response(response_text):\n    \"\"\"Helper to extract raw JSON from potential markdown wrapping.\"\"\"\n    if not response_text: return None\n    clean_text = response_text.strip()\n    if clean_text.startswith(\"```\"):\n        first_newline = clean_text.find(\"\\n\")\n        if first_newline != -1: clean_text = clean_text[first_newline+1:]\n        if clean_text.endswith(\"```\"): clean_text = clean_text[:-3]\n    return clean_text.strip()\n\ndef generate_gold_standard(concepts, lang_name):\n    \"\"\"Generates reference sentences for a list of abstract concepts.\"\"\"\n    if not concepts or not lang_name: return []\n\n    prompt = f\"\"\"\n    Translate the following list of sentences into {lang_name}.\n    Ensure the translation is natural but strictly grammatical.\n    \n    INPUT LIST: {json.dumps(concepts)}\n    \n    INSTRUCTIONS:\n    Return ONLY a raw JSON list of strings. No markdown.\n    Example output: [\"Sentence 1\", \"Sentence 2\"]\n    \"\"\"\n    \n    response = client.generate(prompt)\n    clean_json = _clean_json_response(response)\n    if not clean_json: return []\n\n    try:\n        data = json.loads(clean_json)\n        return data if isinstance(data, list) else []\n    except json.JSONDecodeError:\n        return []\n\ndef evaluate_output(source_concept, generated_text, lang_name):\n    \"\"\"Scores the quality of the generated text against the source concept.\"\"\"\n    if not generated_text: return {\"valid\": False, \"score\": 0, \"error\": \"Empty generation input\"}\n\n    prompt = f\"\"\"\n    Act as a strict linguistic judge for the language: {lang_name}.\n    TASK: Verify if the GENERATED TEXT correctly matches the SOURCE CONCEPT.\n    SOURCE CONCEPT: \"{source_concept}\"\n    GENERATED TEXT: \"{generated_text}\"\n    \n    OUTPUT FORMAT:\n    Return strictly a JSON object with these keys:\n    {{\n        \"valid\": boolean,\n        \"score\": integer (0-10),\n        \"correction\": \"string (the corrected sentence or null if perfect)\"\n    }}\n    \"\"\"\n    \n    response = client.generate(prompt)\n    clean_json = _clean_json_response(response)\n    if not clean_json: return {\"valid\": False, \"score\": 0, \"error\": \"No valid response\"}\n\n    try:\n        return json.loads(clean_json)\n    except json.JSONDecodeError:\n        return {\"valid\": False, \"score\": 0, \"error\": \"JSON parse error\"}\n</syntaxhighlight>\n\n=== 3. The Surgeon (surgeon.py) ===\n'''Entry Point:''' <code>ai_services/surgeon.py</code>\n\nThe Surgeon is the repair agent for the \"Self-Healing\" pipeline. It uses compiler error logs to rewrite broken GF source code.\n\n<syntaxhighlight lang=\"python\">\nimport logging\nfrom . import client\n\nlogger = logging.getLogger(\"ai_services.surgeon\")\n\ndef _clean_gf_response(response_text):\n    if not response_text: return None\n    clean_text = response_text.strip()\n    if clean_text.startswith(\"```\"):\n        first_newline = clean_text.find(\"\\n\")\n        if first_newline != -1: clean_text = clean_text[first_newline+1:]\n        if clean_text.endswith(\"```\"): clean_text = clean_text[:-3]\n    return clean_text.strip()\n\ndef repair_grammar(rgl_code, file_content, error_log):\n    \"\"\"Surgically repairs broken GF code based on compiler error logs.\"\"\"\n    logger.info(f\"ü§ñ Surgeon: Analyzing compilation failure for {rgl_code}...\")\n\n    prompt = f\"\"\"\n    You are an expert in Grammatical Framework (GF) and the Resource Grammar Library (RGL).\n    CONTEXT: I am building a concrete syntax file (Wiki{rgl_code}.gf). The build failed.\n    \n    THE ERROR LOG:\n    {error_log}\n    \n    THE BROKEN CODE:\n    {file_content}\n    \n    YOUR MISSION:\n    1. Analyze the error. \n       - If \"constant not found\", replace high-level API calls with lower-level constructors (e.g. MassNP, UseN).\n       - If type mismatch, adjust parameters to match the expected record.\n    2. Fix the code to make it compile.\n    \n    OUTPUT FORMAT: Return ONLY the full, corrected source code. No Markdown.\n    \"\"\"\n\n    response_text = client.generate(prompt)\n    fixed_code = _clean_gf_response(response_text)\n    \n    if fixed_code and \"concrete\" in fixed_code:\n        return fixed_code\n    else:\n        logger.error(f\"Surgeon failed: AI returned invalid code for {rgl_code}\")\n        return None\n</syntaxhighlight>\n\n=== 4. The Lexicographer (lexicographer.py) ===\n'''Entry Point:''' <code>ai_services/lexicographer.py</code>\n\nThe Lexicographer automates the creation of new dictionaries, handling batch processing to respect token limits.\n\n<syntaxhighlight lang=\"python\">\nimport json\nimport logging\nimport math\nfrom . import client\n\nlogger = logging.getLogger(\"ai_services.lexicographer\")\n\ndef generate_lexicon(words, lang_name, batch_size=20):\n    \"\"\"Generates GF morphology dictionaries for a list of English words.\"\"\"\n    if not words or not lang_name: return {}\n\n    full_lexicon = {}\n    total_batches = math.ceil(len(words) / batch_size)\n    \n    for i in range(total_batches):\n        batch = words[i * batch_size : (i + 1) * batch_size]\n        \n        prompt = f\"\"\"\n        Act as an expert lexicographer for Grammatical Framework (GF).\n        TASK: Generate morphology constructors for: {lang_name}.\n        INPUT WORDS: {\", \".join(batch)}\n        \n        INSTRUCTIONS:\n        1. Identify part of speech (_N, _V, _A).\n        2. Provide the correct GF constructor string (mkN, mkV, etc.).\n        \n        OUTPUT FORMAT: JSON object mapping ID to GF code.\n        Example: {{ \"apple_N\": \"mkN \\\\\"pomme\\\\\"\" }}\n        \"\"\"\n        \n        response = client.generate(prompt)\n        # (Parsing logic omitted for brevity; see implementation)\n        # ... merges batch results into full_lexicon ...\n\n    return full_lexicon\n</syntaxhighlight>\n\n== Pipeline Integration Points ==\n\nThe AI services are hooked into the build and test pipelines at specific failure or validation points.\n\n=== A. Build Orchestrator (build_orchestrator.py) ===\n'''Hook:''' <code>healer.run_healing_round()</code>\n\nThe orchestrator manages the high-level build loop. If the first compilation pass fails, it invokes the Healer. If the Healer reports success (patches applied), it triggers a second compilation pass.\n\n<syntaxhighlight lang=\"python\">\n# 4. COMPILE & HEAL (The Muscle + The Surgeon)\nprint(\"\\n--- [4/4] Compiling (Pass 1) ---\")\nsuccess = compiler.run()\n\n# --- SELF-HEALING LOOP ---\nprint(\"\\nüöë Analyzing Failures for AI Repair...\")\npatched = healer.run_healing_round()\n\nif patched:\n    print(\"\\nüîÑ Updates Applied. Compiling (Pass 2)...\")\n    success = compiler.run()\n</syntaxhighlight>\n\n=== B. The Healer (builder/healer.py) ===\n'''Hook:''' <code>run_healing_round()</code>\n\nThe Healer acts as the bridge between the build system and the AI. It reads the <code>build_failures.json</code> report generated by the compiler, iterates through the failures, and dispatches the Surgeon to fix specific files. It also handles rate limiting to prevent API bans.\n\n=== C. Dynamic Tester (test_gf_dynamic.py) ===\n'''Hook:''' <code>judge.evaluate_output()</code>\n\nDuring runtime testing, the Dynamic Tester calls the Judge to verify the semantic and grammatical accuracy of the generated text. This step is typically restricted to major languages or specific test runs to conserve API credits.\n\n<syntaxhighlight lang=\"python\">\nif AI_AVAILABLE and lang_name in MAJOR_LANGS:\n    verdict = judge.evaluate_output(source_concept, text, lang_name)\n    if verdict.get('valid'):\n        print(f\"‚úÖ Score: {verdict.get('score')}/10\")\n    else:\n        print(f\"‚ö†Ô∏è Fix: {verdict.get('correction')}\")\n</syntaxhighlight>\n\n[[Category:Abstract Wikipedia tools]]"
    },
    {
      "rel_path": "BuildLanguageAndEverythingMatrix.txt",
      "ext": ".txt",
      "size_bytes": 10501,
      "kind": "source",
      "content": "\n\n# üìú System Blueprint: Data-Driven Grammar Orchestration\n\n## 1. The \"Everything Matrix\" Core Logic\n\nThe system's \"Brain\" is not a static configuration file but a dynamic index that must be audited before every build.\n\n* **The Source of Truth**: `data/indices/everything_matrix.json` is the central registry.\n* **Modular Scanning**: Instead of a single script, a suite of scanners in `tools/everything_matrix/` must be run to populate the Matrix.\n* **Zone A (Foundation)**: `rgl_scanner.py` and `rgl_auditor.py` locate and verify RGL source files in `gf-rgl/src`.\n* **Zone B (Lexicon)**: `lexicon_scanner.py` audits `data/lexicon/` for vocabulary coverage.\n* **Zone C (App)**: `app_scanner.py` checks for UI/API readiness.\n\n\n* **The Matrix Builder**: `build_index.py` orchestrates these scanners. If the Matrix returns `null` for a language, the scanners must be re-run from the project root.\n\n## 2. PGF Compilation: The \"Last Man Standing\" Fix\n\nWe discovered that standard `gf -make` commands in a loop overwrite the binary, resulting in a PGF containing only the final language in the list (e.g., `WikiXho`).\n\n* **The Two-Phase Build**:\n1. **Verification Phase**: Use `gf -c -batch` for each language. The `-c` flag generates intermediate `.gfo` files without touching the PGF. The `-batch` flag is mandatory to prevent the terminal from hanging on EOF (User input) prompts for every language.\n2. **Linking Phase**: A single `gf -make` command must then be called, passing the Abstract grammar and **all** valid concrete file paths simultaneously.\n\n\n* **Standardized Output**: The binary must be named `AbstractWiki.pgf` and reside in the `gf/` directory to satisfy the API configuration.\n\n## 3. Path Resolution & Environment Anchoring\n\nThe Orchestrator must be \"Root-Aware\" to bridge the gap between Python and GF.\n\n* **RGL Priority**: The system should prioritize local RGL source files found in `../gf-rgl/src` over system-level installations (e.g., `/usr/share/...`). This ensures Tier 1 languages (English, French, etc.) are compiled from the most recent project source.\n* **Factory Base**: The generated \"Factory\" grammars live in `root/generated/src/`, which is one level up from the `gf/` build directory.\n* **GF Path Arguments**: When calling the GF compiler, the `-path` argument must be a deduped, `os.pathsep`-joined string of the current `gf/` dir, the RGL source dir, and the specific Factory folder for that language.\n\n## 4. API & Semantic Frame Requirements\n\nThe API enforces a strict biographical semantic frame that must be followed for successful generation.\n\n* **Nested Payload Structure**: The endpoint `/api/v1/generate` requires a `language_code` and a `frame` object.\n* **BioFrame Schema**:\n* **`subject`**: Must contain `name` and `gender`.\n* **`properties`**: Must contain `profession` and `nationality`.\n\n\n* **Worker Synchronization**: The `arq` worker loads the PGF into memory only once at startup. **Any update to `AbstractWiki.pgf` requires a manual restart of the worker** (`arq app.workers.worker.WorkerSettings`) to reflect the changes.\n\n## 5. Critical Build stats to Remember\n\n* **Stats Audit**: A successful build of the Tier 1 and Tier 3 languages should result in **30+ valid languages** being linked.\n* **Verification Command**: Always run this Python snippet immediately after a build to verify the binary's contents:\n`python3 -c \"import pgf; print(pgf.readPGF('gf/AbstractWiki.pgf').languages.keys())\"`\n\n---\n\n\n\n\n\n# Technical Documentation: Universal Orchestration Architecture\n\n## 1. Overview\n\nThe **Abstract Wiki Architect** utilizes a data-driven orchestration layer to manage the state and compilation of multilingual grammars. Rather than relying on hardcoded configurations, the system uses a \"Source of Truth\" known as the **Everything Matrix** to track language maturity across the foundation (RGL), vocabulary (Lexicon), and production (Application) layers.\n\n---\n\n## 2. The Everything Matrix\n\nThe Matrix is a centralized JSON artifact located at `data/indices/everything_matrix.json`. It serves as the registry for every supported ISO 639-3 language code and stores metadata including:\n\n* **Tier/Maturity**: Classification (e.g., Tier 1 for RGL-based, Tier 3 for Factory-generated).\n* **Source Paths**: Exact disk locations for grammar source files in `gf-rgl` or `generated/src`.\n* **Maturity Metrics**: Vocabulary coverage, API route availability, and compilation health.\n\n---\n\n## 3. The Modular Scanner Suite\n\nThe Matrix is populated by a suite of specialized auditing tools located in `tools/everything_matrix/`.\n\n### A. Matrix Builder (`build_index.py`)\n\nThe master orchestrator of the audit process. It executes all sub-scanners, aggregates their findings, and writes the final JSON index.\n\n### B. RGL Scanner (`rgl_scanner.py`)\n\nResponsible for \"Zone A\" (Foundation). It scans the `gf-rgl/src` directory to identify mature languages and maps them to their respective ISO codes.\n\n### C. Lexicon Scanner (`lexicon_scanner.py`)\n\nResponsible for \"Zone B\" (Lexicon). It audits the `data/lexicon/` directory to measure vocabulary size and the presence of AI-generated seeds.\n\n### D. Application Scanner (`app_scanner.py`)\n\nResponsible for \"Zone C\" (Application). It verifies if a language has a corresponding frontend profile and active backend API routes.\n\n### E. RGL Auditor (`rgl_auditor.py`)\n\nResponsible for \"Zone D\" (Quality). It performs deep-tissue checks on grammar files to ensure they meet the syntactical requirements for the Abstract Wiki project.\n\n---\n\n## 4. Build Orchestration Logic\n\nThe **Build Orchestrator** (`gf/build_orchestrator.py`) utilizes the findings of the scanners to perform high-fidelity linking of the Grammatical Framework (GF) files.\n\n### Compilation Strategy\n\n1. **Isolated Verification**: The script uses `gf -c -batch` to verify individual languages in isolation, generating intermediate `.gfo` files without overwriting the master binary.\n2. **Global Linking**: Once all languages are verified, a final `-make` command links all valid languages into a single PGF binary: `gf/AbstractWiki.pgf`.\n3. **Path Resolution**: Paths are resolved dynamically by looking for local `gf-rgl/src` folders first, ensuring the build is independent of system-level library configurations.\n\n---\n\n## 5. Maintenance Procedures\n\n### To Refresh the System State:\n\nRun the indexer from the project root:\n\n```bash\npython3 tools/everything_matrix/build_index.py\n\n```\n\n### To Rebuild the PGF Binary:\n\nRun the orchestrator from the `gf/` directory:\n\n```bash\ncd gf && python3 build_orchestrator.py\n\n```\n\n### To Verify Binary Content:\n\n```bash\npython3 -c \"import pgf; print(pgf.readPGF('gf/AbstractWiki.pgf').languages.keys())\"\n\n```\n\n---\n\n\n\n### Final Technical Specification: Abstract Wiki Architect (AWA) Orchestration Layer\n\nThis document summarizes the non-obvious architectural requirements, discovered bottlenecks, and the structural \"Source of Truth\" logic necessary to maintain the Abstract Wiki project.\n\n---\n\n## 1. The Source of Truth: The \"Everything Matrix\"\n\nThe system operates on a data-driven model where the build process is a reflection of the system's audited state.\n\n* **The Registry**: All project metadata resides in `data/indices/everything_matrix.json`.\n* **The Scanning Suite**: The Matrix is populated by a modular suite of tools in `tools/everything_matrix/`:\n* **`rgl_scanner.py`**: Maps Tier 1 mature languages in `gf-rgl/src` to ISO codes.\n* **`lexicon_scanner.py`**: Audits `data/lexicon/` to verify vocabulary seeds and AI-generated content.\n* **`app_scanner.py`**: Checks the application layer for UI profiles and active API routes.\n* **`rgl_auditor.py`**: Performs deep-tissue quality checks on the RGL foundation.\n* **`build_index.py`**: The master script that coordinates all scanners to refresh the Matrix.\n\n\n\n---\n\n## 2. PGF Compilation Strategy\n\nWe identified a critical \"Last Man Standing\" bug where sequential `-make` commands overwrite the binary. The solution is a two-phase compilation process.\n\n* **Phase 1: Isolated Verification (`-c`)**: Individual languages are compiled using the `-c` (compile only) flag to generate intermediate `.gfo` files.\n* **Phase 2: Global Linking (`-make`)**: A single, final call to `gf -make` must include the Abstract grammar and *all* valid concrete files simultaneously to produce a complete PGF.\n* **Automation Requirements**: The `-batch` flag is mandatory in all GF calls to prevent the process from hanging on user-input prompts (EOF/Ctrl+D).\n* **Output Standardization**: The final artifact must be named `AbstractWiki.pgf` and placed in the `gf/` directory to ensure API compatibility.\n\n---\n\n## 3. Path Resolution and Environment Anchoring\n\nThe Orchestrator bridges the gap between the project root and the `gf/` build directory.\n\n* **RGL Priority**: The builder is configured to prioritize local source files in `../gf-rgl/src` over system-level library installations.\n* **Factory Location**: \"Factory\" grammars (Tier 3) are located in `root/generated/src/`, which is one level above the compilation directory.\n* **Dynamic Pathing**: The `-path` argument for GF must be a deduped, separator-joined string including the RGL source, the Abstract directory, and specific Factory subfolders.\n\n---\n\n## 4. Operational Requirements (The \"Grand Test\" Flow)\n\nFor the system to function in a live environment, the following state transitions must occur:\n\n1. **Audit**: Run `python3 tools/everything_matrix/build_index.py` to update the Matrix.\n2. **Build**: Run `python3 gf/build_orchestrator.py` to generate the 30+ language PGF.\n3. **Sync**: **Crucial Step:** The `arq` worker must be restarted (`arq app.workers.worker.WorkerSettings`) to load the new PGF into memory.\n4. **Validate**: Use the semantic payload structure for testing:\n* **Endpoint**: `POST /api/v1/generate`\n* **Payload**: Requires a `language_code` and a `frame` object containing a `subject` (name, gender) and `properties` (profession, nationality).\n\n\n\n---\n\n## 5. Verification Checkpoint\n\nAlways verify the binary's integrity post-build with this command:\n`python3 -c \"import pgf; print(pgf.readPGF('gf/AbstractWiki.pgf').languages.keys())\"`\n*A healthy build must return a list containing both Tier 1 (e.g., `WikiFra`, `WikiEng`) and Tier 3 (e.g., `WikiXho`, `WikiZul`) languages.*\n"
    },
    {
      "rel_path": "DEVELOPER_SETUP.md",
      "ext": ".md",
      "size_bytes": 5120,
      "kind": "markdown",
      "content": "\n# Developer Setup Guide (Hybrid Environment)\n\nThis project uses a **Hybrid Architecture**:\n\n1. **Windows 11:** Source code editing (VS Code), git operations, and Docker Desktop.\n2. **WSL 2 (Ubuntu):** Backend execution, Python environment, and Grammatical Framework (GF) compilation.\n\n**CRITICAL NOTE:** Do not attempt to run the Python backend or GF compilation directly in Windows PowerShell. The required C-libraries (`pgf`) and the grammar binary format are Linux-native.\n\n---\n\n## 1. Prerequisites\n\n* **Windows 10/11** with WSL 2 enabled.\n* **Ubuntu 22.04 or 24.04** installed via Microsoft Store.\n* **Docker Desktop** (configured to use the WSL 2 backend).\n* **VS Code** with the **\"WSL\"** extension installed.\n\n---\n\n## 2. Directory Structure\n\nThe system expects the main grammar file (`Wiki.pgf`) to reside in a `gf/` folder within the repository, and the RGL to be a sibling directory.\n\n**Required Layout:**\n\n```text\n/mnt/c/MyCode/AbstractWiki/\n‚îú‚îÄ‚îÄ abstract-wiki-architect/      <-- This Repository\n‚îÇ   ‚îú‚îÄ‚îÄ .env                      <-- Configuration File\n‚îÇ   ‚îú‚îÄ‚îÄ gf/                       <-- Compiled Grammars Location\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Wiki.pgf              <-- The Active Master Grammar\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îî‚îÄ‚îÄ gf-rgl/                       <-- GF Resource Grammar Library (Source)\n\n```\n\n---\n\n## 3. System Initialization (WSL)\n\nOpen your **Ubuntu/WSL Terminal** (not PowerShell) and run these commands to install the core C-dependencies.\n\n```bash\n# 1. Update packages\nsudo apt update\n\n# 2. Install Python tools, C compiler, and GMP (Required for PGF runtime)\nsudo apt install -y python3-venv python3-dev build-essential libgmp-dev\n\n# 3. Install dos2unix (Crucial for fixing Windows line-ending issues in scripts)\nsudo apt install -y dos2unix\n\n# 4. Install the GF Compiler (Binary)\n# (Assuming the .deb file is in your project root, otherwise download from GF website)\n# wget [https://www.grammaticalframework.org/download/gf-3.12-ubuntu-22.04.deb](https://www.grammaticalframework.org/download/gf-3.12-ubuntu-22.04.deb)\nsudo apt install ./gf-3.12-ubuntu-24.04.deb\n\n# Verify installation\ngf --version\n\n```\n\n---\n\n## 4. Building the Resource Grammar Library (RGL)\n\n*Note: This step is required for the \"Full\" compiler mode. For simple testing with `Wiki.pgf`, you can skip to Step 5.*\n\n1. **Clone the RGL** (if missing):\n```bash\ncd /mnt/c/MyCode/AbstractWiki/\ngit clone [https://github.com/GrammaticalFramework/gf-rgl.git](https://github.com/GrammaticalFramework/gf-rgl.git)\n\n```\n\n\n2. **Fix Line Endings & Build**:\nWindows git cloning often adds `CRLF` characters that break Linux build scripts.\n```bash\ncd gf-rgl\ndos2unix Setup.sh languages.csv\nchmod +x Setup.sh\n\n# Build and Install (Takes ~5-10 minutes)\nsudo ./Setup.sh\n\n```\n\n\n\n---\n\n## 5. Python Environment Setup\n\nPerform all these steps inside the `abstract-wiki-architect` folder in **WSL**.\n\n```bash\ncd /mnt/c/MyCode/AbstractWiki/abstract-wiki-architect\n\n# 1. Create Virtual Env\npython3 -m venv venv\n\n# 2. Activate it\nsource venv/bin/activate\n\n# 3. Upgrade pip\npip install --upgrade pip\n\n# 4. Install Dependencies\n# This installs FastAPI, Arq, and compiles the PGF C-extension locally\npip install -r requirements.txt\n\n```\n\n**Manual PGF Install (If requirements fail):**\nIf `pip install -r requirements.txt` fails on the `pgf` step, run this manually:\n\n```bash\npip install pgf\n\n```\n\n---\n\n## 6. Application Configuration\n\nCreate a file named **`.env`** in the project root (`abstract-wiki-architect/.env`). This bridges the Windows file path to the Linux app.\n\n```ini\n# .env\n# --- Engine Configuration ---\nUSE_MOCK_GRAMMAR=False\n\n# --- Persistence Paths ---\n# Point to the folder containing 'Wiki.pgf'\nFILESYSTEM_REPO_PATH=/mnt/c/MyCode/AbstractWiki/abstract-wiki-architect/gf\n\n# --- Dependencies ---\nGF_LIB_PATH=/mnt/c/MyCode/AbstractWiki/gf-rgl\n\n# --- App Settings ---\nAPP_ENV=development\nLOG_LEVEL=INFO\nSTORAGE_BACKEND=filesystem\nAPI_SECRET=dev-secret-123\n\n```\n\n---\n\n## 7. Running the Application\n\nYou need three terminal tabs (all in WSL).\n\n### Terminal 1: Redis (Message Broker)\n\n```bash\ndocker run -p 6379:6379 redis\n\n```\n\n### Terminal 2: The API Server\n\n```bash\nsource venv/bin/activate\nuvicorn app.main:app --reload --port 8000\n\n```\n\n*Wait for the log: `gf_grammar_loaded .../gf/Wiki.pgf*`\n\n### Terminal 3: The Background Worker\n\n```bash\nsource venv/bin/activate\narq app.workers.worker.WorkerSettings\n\n```\n\n---\n\n## 8. Verification (Smoke Test)\n\nTo confirm the engine is actually working, run this `curl` command (in WSL).\nThis uses the simple `Wiki.pgf` test grammar (John/Apple).\n\n```bash\ncurl -X POST http://localhost:8000/api/v1/generate \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"target_language\": \"kor\", \n  \"semantic_frame\": {\n    \"frame_type\": \"John\",\n    \"subject\": {}, \n    \"meta\": {}\n  }\n}'\n\n```\n\n**Success Response:**\n\n```json\n{\"text\": \"John\", \"lang_code\": \"kor\", \"debug_info\": {...}}\n\n```\n\n"
    },
    {
      "rel_path": "everything_matrix.md",
      "ext": ".md",
      "size_bytes": 5912,
      "kind": "markdown",
      "content": "# Technical Specification: The Everything Matrix\n\n## 1\\. Overview\n\nThe **Everything Matrix** (`everything_matrix.json`) is the central nervous system of the project. It aggregates data from the RGL file system, the application frontend, and the compilation artifacts into a single unified index.\n\n**Purpose:**\n\n1.  **Orchestration:** Provides the \"facts\" needed by the build scripts to choose strategies (High Road vs. Safe Mode).\n2.  **Visibility:** Powers the \"Language Status Dashboard\" on the frontend, visualizing progress (Heatmap).\n3.  **Lifecycle Management:** Tracks every language from \"Planned\" (Level 1) to \"Production\" (Level 10) across 15 distinct architectural blocks.\n\n**Artifact Path:** `data/indices/everything_matrix.json`\n\n-----\n\n## 2\\. JSON Data Schema\n\nThe matrix is a JSON object keyed by the **3-letter Wiki Code** (e.g., `Eng`, `Amh`, `Zho`).\n\n```json\n{\n  \"timestamp\": \"2023-10-27T10:00:00Z\",\n  \"languages\": {\n    \"Eng\": {\n      \"meta\": {\n        \"wiki_code\": \"Eng\",\n        \"rgl_code\": \"Eng\",\n        \"iso_639_3\": \"eng\",\n        \"name\": \"English\",\n        \"family\": \"Germanic\"\n      },\n      \"blocks\": {\n        \"rgl_cat\": 10,\n        \"rgl_noun\": 10,\n        \"rgl_grammar\": 10,\n        \"rgl_syntax\": 10,\n        \"lex_seed\": 8,\n        \"app_profile\": 10\n        // ... (see Section 3 for full list)\n      },\n      \"status\": {\n        \"build_strategy\": \"HIGH_ROAD\",\n        \"overall_maturity\": 9.2,\n        \"is_active\": true\n      }\n    }\n  }\n}\n```\n\n-----\n\n## 3\\. The 15 Data Blocks (Variables)\n\nEach language tracks 15 specific components. The value for each variable is an integer from **0 to 10** (see Maturity Scale).\n\n### **Zone A: RGL Foundation (The Engine)**\n\n*These blocks determine if the language can technically compile.*\n\n| Variable Name | Description | Source / Detection Logic |\n| :--- | :--- | :--- |\n| **`rgl_cat`** | Base Category definitions (`CatX.gf`). | Scanner finds `gf-rgl/src/x/CatX.gf`. |\n| **`rgl_noun`** | Noun morphology rules (`NounX.gf`). | Scanner finds `gf-rgl/src/x/NounX.gf`. |\n| **`rgl_paradigms`** | Constructor functions (`ParadigmsX.gf`). | Scanner finds `gf-rgl/src/x/ParadigmsX.gf`. |\n| **`rgl_grammar`** | Core grammatical structure (`GrammarX.gf`). | Scanner finds `GrammarX.gf` (Physical or Family). |\n| **`rgl_syntax`** | High-level API access (`SyntaxX`). | Scanner finds `api` folder support or specific `SyntaxX` module. |\n\n### **Zone B: Lexicon (The Data)**\n\n*These blocks determine the vocabulary size and richness.*\n\n| Variable Name | Description | Source / Detection Logic |\n| :--- | :--- | :--- |\n| **`lex_seed`** | Initial AI-generated dictionary. | Exists in `data/seeds/{lang}.json`. |\n| **`lex_concrete`** | Compiled GF Dictionary file (`WikiX.gf`). | Scanner finds generated `.gf` file in root. |\n| **`lex_wide`** | Large-scale external import (Wiktionary/PanLex). | Check `data/imports/{lang}_wide.csv`. |\n| **`sem_mappings`** | Abstract-to-Concrete mappings. | Check `semantics/mappings/{lang}.json`. |\n\n### **Zone C: Application (The Interface)**\n\n*These blocks determine if the language works in the User Interface.*\n\n| Variable Name | Description | Source / Detection Logic |\n| :--- | :--- | :--- |\n| **`app_profile`** | Frontend configuration entry. | Exists in `architect_frontend/.../profiles.json`. |\n| **`app_assets`** | UI Assets (Flag icon, localized strings). | Check `public/flags/{iso}.svg`. |\n| **`app_routes`** | Active API endpoint status. | Backend API responds 200 OK for this lang. |\n| **`build_config`** | Build Strategy assignment. | Presence in `rgl_matrix_strategy.json`. |\n\n### **Zone D: Quality (The Status)**\n\n*These blocks represent the health of the language.*\n\n| Variable Name | Description | Source / Detection Logic |\n| :--- | :--- | :--- |\n| **`meta_compile`** | Binary compilation status. | `Wiki.pgf` contains this language. |\n| **`meta_test`** | Unit test pass rate. | Percentage of `tests/lang_test.py` passing. |\n\n-----\n\n## 4\\. The Maturity Scale (0-10)\n\nEvery block is assigned a score based on its state.\n\n  * **0 - ABSENT**: The file or data block is completely missing.\n  * **1 - PLANNED**: Listed in configuration but no physical files exist.\n  * **3 - SCAFFOLDED**: Empty files, folders, or placeholders created.\n  * **5 - DRAFT**: Auto-generated content (e.g., AI Lexicon, `Safe Mode` Grammar). Functional but unverified.\n  * **7 - BETA**: Manually reviewed or successfully compiled in `High Road` mode.\n  * **8 - PRE-FINAL**: Fully integrated into the app and passing basic tests.\n  * **10 - FINAL**: Production-ready, validated by a human linguist, 100% test coverage.\n\n-----\n\n## 5\\. Aggregation Logic (Tools)\n\n### **The Aggregator (`tools/everything_matrix/build_index.py`)**\n\nThis script acts as the \"Grand Aggregator.\" It calls the sub-scanners, consolidates scores, and generates the final JSON.\n\n1.  **Reads RGL Inventory:** Maps `rgl_*` scores (0 if missing, 10 if present) using `tools/everything_matrix/rgl_scanner.py`.\n2.  **Reads Strategy Map:** If Strategy is `HIGH_ROAD`, `rgl_syntax` gets 10. If `SAFE_MODE`, it gets 5.\n3.  **Reads File System:** Checks for flags, JSON seeds, and generated GF files via dedicated scanners (`lexicon_scanner.py`, `app_scanner.py`).\n4.  **Reads PGF:** Queries the compiled `Wiki.pgf` to confirm `meta_compile` status.\n5.  **Outputs:** Writes the consolidated JSON to `data/indices/everything_matrix.json`.\n\n-----\n\n## 6\\. Visualization (The Dashboard)\n\nThe Frontend will read `everything_matrix.json` to render the **Language Health Grid**:\n\n  * **Rows:** Languages.\n  * **Columns:** The 15 variables above.\n  * **Cell Color:**\n      * `0-2`: üî¥ Red (Missing/Blocker)\n      * `3-5`: üü° Yellow (Draft/Safe Mode)\n      * `6-8`: üîµ Blue (Functional/Beta)\n      * `9-10`: üü¢ Green (Production)"
    },
    {
      "rel_path": "Glossary.md",
      "ext": ".md",
      "size_bytes": 5228,
      "kind": "markdown",
      "content": "Here is the **Master Glossary** for the Abstract Wiki Architect (V2). This consolidates all the concepts discussed into a single reference guide, organized by function.\n\n---\n\n### üèóÔ∏è 1. The Architecture (\"The Refinery\")\n*The high-level design that treats language generation as a manufacturing pipeline.*\n\n* **The Refinery:** The overall V2 architecture. It accepts raw language data, processes it through different tiers of quality, and outputs a usable grammar database.\n* **Tier 1 (RGL - Gold Standard):** Languages supported by the official *Grammatical Framework Resource Grammar Library*. These have perfect morphology and syntax (e.g., English, French, Finnish).\n* **Tier 2 (Contrib - Silver/Manual):** Languages that started as Tier 3 but were improved by AI or humans. These files live in `gf/contrib/` and override Tier 3.\n* **Tier 3 (Factory - Bronze/Pidgin):** Languages generated programmatically by `grammar_factory.py`. They use simple string concatenation (SVO word order) to ensure 100% API coverage, even if the grammar is rough.\n* **The Orchestrator:** The `gf/build_orchestrator.py` script. It acts as the project manager, deciding which Tier to use for each language and compiling them into the final database.\n\n---\n\n### üß† 2. Grammatical Framework (\"The Engine\")\n*The specific software technology (GF) powering the linguistics.*\n\n* **Abstract Grammar (`AbstractWiki.gf`):** The \"Interface.\" It defines *what* can be said (functions like `mkFact` and categories like `Entity`) but not *how* to say it. It is language-independent.\n* **Concrete Grammar (`WikiEng.gf`, `WikiZul.gf`):** The \"Implementation.\" It maps the Abstract rules to specific words and word orders for a specific language.\n* **PGF (`Wiki.pgf`):** *Portable Grammar Format*. The compiled binary file (like a `.exe` or `.dll`). The Python API loads this file to generate text.\n* **Linearization:** The process of turning a semantic tree (Abstract) into a text string (Concrete).\n* **Smart Paradigms (`mkN`, `mkV`):** \"Magic\" functions in the RGL that try to guess all forms of a word (plural, past tense, etc.) based on just one dictionary form. (The source of the Russian build error).\n* **Coercion:** A linguistic trick used to force one category into another (e.g., turning a Noun Phrase into an Adverbial Phrase using a Preposition).\n\n---\n\n### ‚öôÔ∏è 3. The Build Process (\"The Assembly Line\")\n*How code turns into the runnable engine.*\n\n* **Vocabulary Stubs:** A minimal list of words (like \"animal\" and \"walk\") used to test if a language module is valid during the build.\n* **Permissive Mode:** A build strategy where the Orchestrator skips languages that fail to compile (logging the error), rather than stopping the entire process.\n* **Pidgin:** The output style of Tier 3 languages. It lacks complex grammar (agreement, conjugation) but conveys meaning clearly (e.g., \"Shaka IS Warrior\").\n* **Build Logs:** JSON or text files generated during the build that detail exactly why a specific language failed, used for debugging.\n\n---\n\n### üóÑÔ∏è 4. Data Sources & Semantics (\"The Inputs\")\n*The raw information fed into the system.*\n\n* **Wikidata (Q-Items):** The global knowledge base used as the source of truth. `Q42` -> *Douglas Adams*.\n* **Abstract Wikipedia (Z-Objects):** The theoretical format for language-independent content. Our system is a renderer for this concept.\n* **Frame:** The JSON object sent by the frontend (e.g., `{ \"type\": \"bio\", \"name\": \"Shaka\" }`). It represents the *meaning* the user wants to express.\n* **Ontology:** The map of \"what exists.\" Defined by our schemas (e.g., a \"Person\" has a \"Birth Date\").\n* **AST (Abstract Syntax Tree):** The strict, tree-like structure required by GF. The \"Frame\" is converted into an \"AST\" before generation.\n* **Lexicon:** The dictionary database mapping concepts (Q-Items) to words in target languages.\n\n---\n\n### üíª 5. Runtime & API (\"The Application\")\n*The live software running on the server.*\n\n* **NLG Client:** The Python service (`services/nlg_client.py`) that holds the PGF file in memory and answers API requests.\n* **Bridge (The Mapper):** The logic (`semantics/aw_bridge.py`) that converts the user's JSON Frame into a GF Abstract Syntax Tree.\n* **Real-Time Realization:** The feature where text updates instantly in 300 languages as the user types.\n* **Hot-Reloading:** The ability of the server to detect a new `Wiki.pgf` (after a build) and load it without crashing.\n* **Endpoint:** A specific URL function, e.g., `POST /generate` (make text) or `POST /grammar/refine` (fix code).\n\n---\n\n### üß™ 6. Quality & Refinement (\"The Loop\")\n*How the system improves over time.*\n\n* **Smoke Test:** A basic test (`test_gf_dynamic.py`) to ensure the engine starts and can generate *something*.\n* **Gold Standard:** A list of human-written \"perfect\" translations used to benchmark the system's output.\n* **AI Refiner:** An offline tool (`utils/ai_refiner.py`) that uses an LLM (Gemini) to read a crude Tier 3 grammar and rewrite it as a better Tier 2 grammar.\n* **Repair Ticket:** The concept of flagging a specific language error so the AI Refiner can attempt to fix it in the next build cycle."
    },
    {
      "rel_path": "HybridEnvironment-LinuxWindows.md",
      "ext": ".md",
      "size_bytes": 3090,
      "kind": "markdown",
      "content": "### The Linux Environment Solution (WSL)\n\nYou are using **WSL 2 (Windows Subsystem for Linux)** running an **Ubuntu** distribution.\n\n  * **What it is:** It acts like a \"Virtual Machine\" integrated directly into Windows, but without the slowness or isolation of a traditional VM. It gives you a real Linux kernel and terminal command line.\n  * **Why we need it:** Your project relies on the **Grammatical Framework (GF)** library (`pgf`). This library is built with C++ and is native to Linux. Installing it on Windows requires complex compiler setups (Visual Studio Build Tools) that often fail. In Linux (Ubuntu), we just ran `apt install build-essential` and it worked instantly.\n  * **How it works:**\n      * Your code lives on your Windows hard drive (`C:\\MyCode\\...`).\n      * WSL \"mounts\" your C: drive so Linux can see it at `/mnt/c/`.\n      * You edit files in Windows (VS Code, Notepad, etc.).\n      * You **run** the backend in the Linux terminal because that's where the Python environment with the compiled C++ libraries lives.\n\n-----\n\n### üìù Context to Carry Forward (Copy-Paste this for your next chat)\n\nWhen you start a new conversation, paste this block so the AI knows exactly where you stand:\n\n> **Project Context: Abstract Wiki Architect**\n>\n> **1. Environment Setup (Hybrid):**\n>\n>   * **OS:** Windows 11 with **WSL 2 (Ubuntu)** installed.\n>   * **Code Location:** `C:\\MyCode\\AbstractWiki\\abstract-wiki-architect` (mapped to `/mnt/c/...` in WSL).\n>   * **Backend:** Runs inside **WSL** (Ubuntu) to support the `pgf` C++ library.\n>       * *Command:* `source venv/bin/activate` -\\> `uvicorn architect_http_api.main:app --reload --port 8000 --host 0.0.0.0`\n>   * **Frontend:** Runs in **Windows PowerShell** (standard Next.js).\n>       * *Command:* `npm run dev` (Port 3000).\n>\n> **2. Current Code State:**\n>\n>   * **GF Engine:** The file `architect_http_api/gf/__init__.py` is set to **load** the real engine (we reverted the \"disable\" patch).\n>   * **Dependencies:** `pgf` and `google-generativeai` are installed in the WSL `venv`.\n>   * **Configuration:** `language_profiles/profiles.json` uses **3-letter ISO codes** (e.g., `fra`, `eng`) to match the API.\n>   * **Lexicons:** We have a script `utils/seed_lexicon_ai.py` to generate dictionaries using Gemini, bypassing the need for the full RGL build pipeline if necessary.\n>\n> **3. Immediate Goal:**\n>\n>   * We are verifying that the backend (in WSL) correctly generates text using the Python engines (Romance, Germanic) and the GF engine.\n\n-----\n\n### How to Resume Work Next Time\n\n**1. Start the Frontend (Windows Terminal)**\n\n```powershell\ncd C:\\MyCode\\AbstractWiki\\abstract-wiki-architect\\architect_frontend\nnpm run dev\n```\n\n**2. Start the Backend (Ubuntu/WSL Terminal)**\n\n```bash\n# Enter Linux\nwsl\n\n# Go to project (Windows C: is /mnt/c in Linux)\ncd /mnt/c/MyCode/AbstractWiki/abstract-wiki-architect\n\n# Activate Python Virtual Env\nsource venv/bin/activate\n\n# Run Server\nuvicorn architect_http_api.main:app --reload --port 8000 --host 0.0.0.0\n```"
    },
    {
      "rel_path": "Overview.md",
      "ext": ".md",
      "size_bytes": 5608,
      "kind": "markdown",
      "content": "\n\n# üèõÔ∏è Abstract Wiki Architect: System Documentation\n\n### **1. üß† Data Centralization (The Source of Truth)**\n\nThese files hold the configuration, paths, and definitions. They are the \"Brain\" of the system. If you want to add a language or change a setting, you edit these.\n\n| File Path | Role | Description |\n| :--- | :--- | :--- |\n| **`rgl_paths.json`** | **Ground Truth** | Generated by `generate_path_map.py`. It maps every available RGL language module on your disk to its specific file path. **`builder/config.py`** reads this to know which languages exist. |\n| **`builder/config.py`** | **The Blueprint** | The central configuration script. It loads `rgl_paths.json`, defines the **Ambiguity Strategy** (e.g., `indef`), handles ISO-to-RGL code mapping (e.g., `Deu` -\\> `Ger`), and defines shared RGL families. |\n| **`language_profiles/profiles.json`** | **Frontend Config** | Holds metadata for the API/Frontend (e.g., \"French belongs to the Romance family\"). This file is updated automatically by `sync_config_from_gf.py` after a build. |\n| **`data/lexicon/`** | **Lexical Data** | A directory containing JSON shards (e.g., `people.json`, `science.json`) generated from Wikidata. This is the raw vocabulary that `forge.py` will eventually read to build the lexicon. |\n\n-----\n\n### **2. üèóÔ∏è The Builder Core (Construction)**\n\nThese files are responsible for taking the \"Truth\" and converting it into a working Grammar Binary (`.pgf`).\n\n| File Path | Role | Description |\n| :--- | :--- | :--- |\n| **`build_orchestrator.py`** | **The Boss** | The entry point. You run this script. It coordinates the entire workflow: it calls `forge.py` to write code, then `compiler.py` to build it. |\n| **`builder/forge.py`** | **The Factory** | The code generator. It reads `config.py` and `rgl_paths.json`, deletes old/broken `.gf` files, and writes new ones using your chosen architectural strategies (e.g., enforcing `mkNP a_Det cn`). |\n| **`builder/compiler.py`** | **The Worker** | The interface to the GF binary. It constructs the complex path arguments (handling WSL/Windows paths), runs the `gf -make` commands, and links the final `Wiki.pgf`. |\n| **`builder/__init__.py`** | **Package Marker** | An empty file that tells Python to treat the `builder` folder as a module, allowing imports like `from builder import forge`. |\n\n-----\n\n### **3. üî¨ Analysis & Verification (Quality Control)**\n\nThese files run *after* the build to ensure the system is working correctly and to synchronize the new grammar with the rest of the application.\n\n| File Path | Role | Description |\n| :--- | :--- | :--- |\n| **`test_gf_dynamic.py`** | **Grammar Test** | Dynamically loads the new `Wiki.pgf`, detects all 50+ languages, and attempts to linearize a test sentence (e.g., \"A cat\"). It flags any language that fails to produce text. |\n| **`check_all_languages.py`** | **API Test** | Tests the running Python API server. It sends a request for every supported language to ensure the web server is correctly loading the new grammar. |\n| **`sync_config_from_gf.py`** | **The Bridge** | Extracts linguistic data (like Copulas) from the compiled `Wiki.pgf` and updates the JSON files in `data/` and `language_profiles/`, ensuring the frontend matches the backend. |\n| **`audit_languages.py`** | **Deep Audit** | (Optional/Debug) Runs a slow, individual compilation of every language to isolate specific errors if the main build crashes. |\n\n-----\n\n### **4. üõ†Ô∏è Utilities & Helpers**\n\nStandalone tools for specific tasks.\n\n| File Path | Role | Description |\n| :--- | :--- | :--- |\n| **`generate_path_map.py`** | **Setup Tool** | Scans your `gf-rgl` directory and generates the `rgl_paths.json` file. You only run this once (or when you install new RGL languages). |\n| **`build_lexicon_from_wikidata.py`** | **Data Miner** | Connects to Wikidata to fetch thousands of words (Scientists, Planets, etc.) and saves them to `data/lexicon/`. This feeds the system with new vocabulary. |\n| **`cleanup_root.py`** | **Janitor** | Moves loose `.gf` files into `gf/` and deletes `.gfo` compiler artifacts to keep the root directory clean. |\n\n-----\n\n### **üìÇ System Directory Map**\n\n\nroot/\n‚îú‚îÄ‚îÄ build_orchestrator.py        <-- [RUN THIS] to build everything\n‚îú‚îÄ‚îÄ build_pipeline.bat           <-- [OR THIS] Batch wrapper for the pipeline\n‚îú‚îÄ‚îÄ rgl_paths.json               <-- [DATA] The map of installed languages\n‚îÇ\n‚îú‚îÄ‚îÄ builder/                     <-- [MODULE] The Build System\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ config.py                <-- [CONFIG] Central settings & logic\n‚îÇ   ‚îú‚îÄ‚îÄ forge.py                 <-- [LOGIC] Code generator\n‚îÇ   ‚îî‚îÄ‚îÄ compiler.py              <-- [LOGIC] GF Compiler interface\n‚îÇ\n‚îú‚îÄ‚îÄ gf/                          <-- [OUTPUT] Where .gf and .pgf files live\n‚îÇ   ‚îú‚îÄ‚îÄ Wiki.pgf                 <-- The final compiled product\n‚îÇ   ‚îî‚îÄ‚îÄ (Generated .gf files)\n‚îÇ\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îî‚îÄ‚îÄ lexicon/                 <-- [DATA] Raw words from Wikidata\n‚îÇ\n‚îî‚îÄ‚îÄ utils/                       <-- (Suggested) Place for audit/cleanup scripts\n    ‚îú‚îÄ‚îÄ test_gf_dynamic.py\n    ‚îî‚îÄ‚îÄ check_all_languages.py\n```\n\n### **üöÄ How to Run the Pipeline**\n\n**1. One-Click Build:**\nRun `build_pipeline.bat` (Windows) or:\n\n```\npython build_orchestrator.py\n```\n\n**2. Update Lexicon (When adding words):**\n\n```\npython build_lexicon_from_wikidata.py\npython build_orchestrator.py\n```\n\n**3. Verify System:**\n\n```\npython test_gf_dynamic.py\n```"
    },
    {
      "rel_path": "SeniorUpdate.txt",
      "ext": ".txt",
      "size_bytes": 14129,
      "kind": "source",
      "content": "This is a comprehensive architectural assessment. To achieve the \"Greatest App Ever\" status, you cannot simply collect patterns like Pok√©mon. You must select a cohesive set that balances agility, reliability, and scalability.\n\nHere is the Master Plan‚Äîa strategic evaluation of all senior architecture patterns found in your notebooks, categorized by what you must Adopt, Refine, and Avoid.\n\nI. Structural Backbone: The Foundation\nThe goal is maintainability and preventing a \"Big Ball of Mud.\"\n\nPattern\tVerdict\tExpert Analysis\nDomain-Driven Design (DDD)\tMUST IMPLEMENT\tThis is non-negotiable for a top-tier app. You must model your software around the business domain (Bounded Contexts), not the database schema. This ensures your code speaks the same language as your business experts.\nHexagonal Architecture (Ports & Adapters)\tMUST IMPLEMENT\tEssential for testing and flexibility. It isolates your core logic from the \"dirty\" outside world (databases, UIs, APIs). If you want to swap SQL for NoSQL later, this pattern makes it painless.\nMicroservices\tREFINE / CAUTION\tDo not start here unless you have 50+ developers. Start with a Modular Monolith (using DDD/Hexagonal). When a specific module (e.g., \"Billing\") needs independent scaling, then break it out. Premature microservices create a \"Distributed Monolith\" nightmare.\nSidecar Pattern\tADOPT (Later)\tUseful for offloading cross-cutting concerns like logging or SSL termination to a separate container, keeping your application code clean. Implement this when you move to Kubernetes/Service Mesh.\n\nExport to Sheets\n\nII. Data Management: The Source of Truth\nThe goal is consistency and integrity across a distributed system.\n\nPattern\tVerdict\tExpert Analysis\nDatabase-per-Service\tMUST IMPLEMENT\tIf you split services, they cannot share a database. Shared databases create tight coupling‚Äîif one team changes a schema, everyone else breaks. This is a hard rule for senior architecture.\nSaga Pattern\tMUST IMPLEMENT\tAs mentioned before, you need this for distributed transactions. Avoid 2-Phase Commit (2PC). Use Orchestration-based Sagas for complex flows to keep logic centralized and manageable.\nCQRS (Command Query Responsibility Segregation)\tSELECTIVE ADOPTION\tDon't use this everywhere. Use it only for high-traffic parts of the app where read patterns differ vastly from write patterns (e.g., a social media feed vs. a user profile update). Using it everywhere doubles your complexity.\nEvent Sourcing\tAVOID (Mostly)\tUnless you are building a bank or a legal ledger that requires a replayable history of every state change, avoid this. It adds immense complexity to debugging and versioning. Standard CRUD + Audit Logs is usually sufficient.\n\nExport to Sheets\n\nIII. Messaging & Communication: The Nervous System\nThe goal is decoupling and handling load spikes.\n\nPattern\tVerdict\tExpert Analysis\nAsync Messaging (Pub/Sub)\tMUST IMPLEMENT\tSynchronous HTTP calls chain failure. If Service A calls B, which calls C, and C hangs, everything hangs. Use message queues (Kafka/RabbitMQ) for everything that doesn't need an immediate answer (e.g., sending emails, processing analytics).\nAPI Gateway\tMUST IMPLEMENT\tDo not let clients talk directly to your backend services. The Gateway handles authentication, rate limiting, and routing. It is your single point of entry and defense.\nBackends for Frontends (BFF)\tADOPT\tCreate specific API layers for different clients (Mobile vs. Web). Mobile apps need less data to save bandwidth; Web apps can handle more. Don't force a \"one size fits all\" API on different devices.\n\nExport to Sheets\n\nIV. Stability & Resilience: The Shield\nThe goal is to stay up when everything else goes down.\n\nPattern\tVerdict\tExpert Analysis\nCircuit Breaker\tMUST IMPLEMENT\tEssential. Prevents cascading failures. Ideally, use a library (like Resilience4j or Polly) rather than writing this yourself.\nBulkhead Pattern\tADOPT\tCritical for high-load systems. Ensure a failure in the \"Reporting\" module doesn't consume all thread pools and kill the \"Checkout\" module.\nRate Limiting / Throttling\tMUST IMPLEMENT\tProtect your API from abuse and traffic spikes. Implement this at the API Gateway level (Leaky Bucket or Token Bucket algorithms).\nRetry with Exponential Backoff\tREFINE\t\"Retry\" is dangerous if done blindly (it causes retry storms). Always use Exponential Backoff (wait longer after each fail) and Jitter (randomize the wait) to prevent thundering herds.\n\nExport to Sheets\n\nV. Observability & Operations: The Eyes\nThe goal is to fix problems before users report them.\n\nPattern\tVerdict\tExpert Analysis\nDistributed Tracing\tMUST IMPLEMENT\tIn a distributed system, logs are useless without context. You need a Trace ID that follows a request across all services (OpenTelemetry is the standard).\nStructured Logging\tMUST IMPLEMENT\tLogs must be data (JSON), not text strings. This allows you to query \"Show me all errors for UserID X\" instantly in tools like ELK or Splunk.\nCanary Deployments\tADOPT\tDon't deploy to 100% of users at once. Deploy to 5%, monitor metrics, and then roll out. This catches bugs with minimal impact.\nChaos Engineering\tADOPT (Later)\tIntentionally breaking things in production to test resilience. This is for very mature teams (Netflix/Amazon level). Don't do this until your basic automated testing is rock solid.\n\nExport to Sheets\n\nThe \"Greatest App\" Master Plan Visualization\nStrategic Summary:\n\nStart with a Modular Monolith using DDD and Hexagonal Architecture.\n\nUse an API Gateway fronting your app.\n\nImplement Async Messaging early to keep modules decoupled.\n\nAdopt Database-per-Module logical separation immediately (even if on one physical DB server).\n\nLayer in Circuit Breakers and Distributed Tracing from Day 1.\n\nWait on Microservices, Event Sourcing, and Chaos Engineering until scale demands it.\n\nThis approach gives you the structure of a giant tech company without the crippling complexity of managing it before you are ready.\n\n\n\n\nThis step defines the **System Constants and Configuration Contract**. By establishing these files first, we ensure that every subsequent file (Adapters, Use Cases, Tests) imports the exact same variable names, types, and defaults.\n\nHere is the code that aligns the **Infrastructure** (Docker/Env), **Application** (Config), and **Domain** (Models/Events).\n\n### 1\\. Project Dependencies (`pyproject.toml`)\n\n*Defines the library versions (variables) used across the system.*\n\n```toml\n[tool.poetry]\nname = \"abstract-wiki-architect\"\nversion = \"2.0.0\"\ndescription = \"Industrial-grade NLG system for Abstract Wikipedia\"\nauthors = [\"Architecture Team\"]\n\n[tool.poetry.dependencies]\npython = \"^3.10\"\nfastapi = \"^0.109.0\"\nuvicorn = {extras = [\"standard\"], version = \"^0.27.0\"}\npydantic = \"^2.6.0\"\npydantic-settings = \"^2.1.0\"\nstructlog = \"^24.1.0\"\nopentelemetry-api = \"^1.22.0\"\nopentelemetry-sdk = \"^1.22.0\"\nopentelemetry-instrumentation-fastapi = \"^0.43b0\"\nredis = \"^5.0.1\"\narq = \"^0.25.0\"  # Async Task Queue\ntenacity = \"^8.2.3\" # Circuit Breakers & Retries\nhttpx = \"^0.26.0\"\njinja2 = \"^3.1.3\"\n\n[tool.poetry.group.dev.dependencies]\npytest = \"^8.0.0\"\npytest-asyncio = \"^0.23.0\"\nblack = \"^24.1.0\"\nmypy = \"^1.8.0\"\nhttpx = \"^0.26.0\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n```\n\n### 2\\. Environment Variables (`.env.example`)\n\n*The contract between the OS and the App. These variable names MUST match `app/shared/config.py`.*\n\n```bash\n# --- Application ---\nAPP_NAME=abstract-wiki-architect\nAPP_ENV=development # development, staging, production\nDEBUG=true\nLOG_LEVEL=INFO\nLOG_FORMAT=json # console, json\n\n# --- Persistence ---\n# Path to the local lexicon storage (for dev)\nFILESYSTEM_REPO_PATH=/app/data\n\n# --- Messaging (Redis) ---\n# Hostname matches the service name in docker-compose.yml\nREDIS_HOST=redis\nREDIS_PORT=6379\nREDIS_DB=0\nREDIS_QUEUE_NAME=architect_tasks\n\n# --- External Services ---\nWIKIDATA_SPARQL_URL=https://query.wikidata.org/sparql\n# Timeout in seconds before Circuit Breaker trips\nWIKIDATA_TIMEOUT=30\n\n# --- Worker ---\n# Max concurrent builds\nWORKER_CONCURRENCY=2\n```\n\n### 3\\. Infrastructure Orchestration (`docker-compose.yml`)\n\n*Aligns hostnames (`redis`, `backend`) with the Environment Variables.*\n\n```yaml\nversion: '3.8'\n\nservices:\n  backend:\n    build:\n      context: .\n      dockerfile: docker/Dockerfile.backend\n    container_name: aw_backend\n    ports:\n      - \"8000:8000\"\n    env_file: .env\n    depends_on:\n      - redis\n    volumes:\n      - .:/app\n    command: uvicorn app.adapters.api.main:app --host 0.0.0.0 --port 8000 --reload\n\n  worker:\n    build:\n      context: .\n      dockerfile: docker/Dockerfile.worker\n    container_name: aw_worker\n    env_file: .env\n    depends_on:\n      - redis\n    volumes:\n      - .:/app\n    # Starts the arq worker listening on the specific Redis Settings\n    command: arq app.worker.main.WorkerSettings\n\n  redis:\n    image: redis:alpine\n    container_name: aw_redis\n    ports:\n      - \"6379:6379\"\n```\n\n### 4\\. Application Configuration (`app/shared/config.py`)\n\n*The Single Source of Truth. All Python code imports settings from here, never from `os.environ` directly.*\n\n```python\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom enum import Enum\n\nclass AppEnv(str, Enum):\n    DEVELOPMENT = \"development\"\n    PRODUCTION = \"production\"\n    TESTING = \"testing\"\n\nclass Settings(BaseSettings):\n    # --- Meta ---\n    APP_NAME: str = \"Abstract Wiki Architect\"\n    APP_ENV: AppEnv = AppEnv.DEVELOPMENT\n    DEBUG: bool = False\n    \n    # --- Logging ---\n    LOG_LEVEL: str = \"INFO\"\n    LOG_FORMAT: str = \"json\"\n\n    # --- Redis ---\n    REDIS_HOST: str = \"localhost\"\n    REDIS_PORT: int = 6379\n    REDIS_DB: int = 0\n    REDIS_QUEUE_NAME: str = \"architect_tasks\"\n\n    @property\n    def redis_url(self) -> str:\n        return f\"redis://{self.REDIS_HOST}:{self.REDIS_PORT}/{self.REDIS_DB}\"\n\n    # --- External ---\n    WIKIDATA_SPARQL_URL: str = \"https://query.wikidata.org/sparql\"\n    WIKIDATA_TIMEOUT: int = 30\n    \n    # --- Paths ---\n    FILESYSTEM_REPO_PATH: str = \"/app/data\"\n\n    model_config = SettingsConfigDict(env_file=\".env\", extra=\"ignore\")\n\n# Singleton instance\nsettings = Settings()\n```\n\n### 5\\. Domain Data Structures (`app/core/domain/models.py`)\n\n*Defines the variable names for data passing through the system.*\n\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Dict, Any, Optional, List\nfrom enum import Enum\nfrom datetime import datetime\n\nclass LanguageStatus(str, Enum):\n    PLANNED = \"planned\"\n    BUILDING = \"building\"\n    READY = \"ready\"\n    ERROR = \"error\"\n\nclass Language(BaseModel):\n    \"\"\"Core entity representing a supported language.\"\"\"\n    code: str  # ISO 639-3 (e.g., 'fra', 'zul')\n    name: str\n    status: LanguageStatus = LanguageStatus.PLANNED\n    grammar_engine: str = \"gf\"  # 'gf', 'python', 'neural'\n    meta: Dict[str, Any] = Field(default_factory=dict)\n\nclass Frame(BaseModel):\n    \"\"\"The input semantic frame.\"\"\"\n    frame_type: str # e.g. 'bio', 'event'\n    subject: Dict[str, Any]\n    properties: Dict[str, Any] = Field(default_factory=dict)\n\nclass Sentence(BaseModel):\n    \"\"\"The output generation.\"\"\"\n    text: str\n    lang_code: str\n    meta: Dict[str, Any] = Field(default_factory=dict)\n```\n\n### 6\\. Event Vocabulary (`app/core/domain/events.py`)\n\n*Aligns the variable names for Async Messaging. Use these constants, never raw strings.*\n\n```python\nfrom enum import Enum\nfrom pydantic import BaseModel\nfrom typing import Dict, Any\n\nclass EventType(str, Enum):\n    BUILD_REQUESTED = \"language.build.requested\"\n    BUILD_COMPLETED = \"language.build.completed\"\n    BUILD_FAILED = \"language.build.failed\"\n    LEXICON_UPDATED = \"lexicon.updated\"\n\nclass SystemEvent(BaseModel):\n    \"\"\"Standard envelope for all async messages.\"\"\"\n    type: EventType\n    payload: Dict[str, Any]\n    trace_id: str\n    timestamp: float\n```\n\n\nYes, this file list covers **100% of the architectural requirements** defined in the Master Plan.\n\nHere is the verification of how these files map to the strategic goals:\n\n### \\u2705 1. Hexagonal Structure (The Core)\n* **Domain Isolation:** You have `app/core/domain` for pure logic (no frameworks).\n* **Interface Enforcement:** `app/core/ports` ensures the Core never talks directly to Redis or GF; it talks to `IGrammarEngine` or `IMessageBroker`.\n* **Dependency Injection:** `app/shared/container.py` wires the `adapters` to the `ports`, enabling the modularity we promised.\n\n### \\u2705 2. Resilience (The Shield)\n* **Circuit Breakers:** Implemented in `app/shared/resilience.py` and applied in `app/adapters/persistence/wikidata_adapter.py`.\n* **Bulkheading:** Achieved via `docker/Dockerfile.worker` (isolating compilation resources) and `app/worker/` (separate process).\n\n### \\u2705 3. Data & Consistency (The Nervous System)\n* **Async Messaging:** Defined in `app/core/domain/events.py` (the contract) and implemented in `app/adapters/messaging/redis_broker.py` (the plumbing).\n* **Sagas:** The complex orchestration is captured in `app/core/use_cases/onboard_language_saga.py`, handling the multi-step build process.\n\n### \\u2705 4. Observability (The Eyes)\n* **Tracing & Logging:** `app/shared/observability.py` and `app/shared/logging_config.py` ensure that every request through `app/adapters/api` and every task in `app/worker` carries a Trace ID and emits JSON logs.\n\n### \\u26a0\\ufe0f One Minor Recommendation\nWhile strictly not \"missing\" (as `arq` handles job state in Redis and `filesystem_repo` handles data), a truly \"Senior\" system often includes a **Database Migration Tool** (like `alembic`) if you plan to store user accounts or complex metadata in a SQL database later.\n\nHowever, for this specific **Language Architecture** upgrade (which relies heavily on file-based grammars and Redis job queues), **the list is complete and ready for execution.**\n\n"
    }
  ]
}