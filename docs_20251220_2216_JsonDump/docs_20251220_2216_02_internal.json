{
  "format": "wiki_dump_json",
  "format_version": 1,
  "generated_at": "2025-12-20T22:16",
  "title": "FOLDER: internal",
  "root_dir": "C:\\MyCode\\AbstractWiki\\abstract-wiki-architect\\docs",
  "stats": {
    "file_count": 4,
    "total_size_bytes": 44939,
    "total_size_mb": 0.0429
  },
  "nav": {
    "home_file": null,
    "prev_file": "docs_20251220_2216_01_ROOT.json",
    "next_file": null,
    "prev_title": "Root",
    "next_title": ""
  },
  "files": [
    {
      "rel_path": "internal/DECISION_LOG.md",
      "ext": ".md",
      "size_bytes": 10909,
      "kind": "markdown",
      "content": "# DESIGN_DECISIONS.md  \nAbstract Wiki Architect ‚Äì Design Decisions\n\nThis document records the **key architectural choices** behind Abstract Wiki Architect\nand the alternatives that were considered. It is meant to be a concise ‚Äúwhy we did it\nthis way‚Äù reference for reviewers and future contributors.\n\n---\n\n## 1. Overall System Shape\n\n### Decision: Router ‚Üí Family Engine ‚Üí Constructions ‚Üí Morphology ‚Üí Lexicon\n\n**What we do**\n\n- The pipeline is:\n\n  1. **Router** (`router.py`) chooses:\n     - family engine,\n     - language profile,\n     - morphology config,\n     - lexicon.\n  2. **Semantics + Discourse** build a language-independent frame:\n     - e.g. `BioFrame`, plus `DiscourseState`.\n  3. **Constructions** choose a clause pattern for the frame.\n  4. **Family Engine** realizes words + syntax using:\n     - family matrix JSON,\n     - language card JSON,\n     - lexicon entries.\n  5. **Morphology** handles inflection, agreement, phonology details.\n  6. **Lexicon subsystem** provides lemma-level information and IDs.\n\n**Alternatives considered**\n\n1. **Per-language renderers** (one function per language, no shared engine)\n\n   - Pros:\n     - Simple to start with.\n     - Easy to hack per-language behavior.\n   - Cons:\n     - Massive code duplication.\n     - Hard to keep consistent across 100+ languages.\n     - Any bug fix must be copied by hand many times.\n\n2. **Single monolithic ‚Äúuniversal‚Äù renderer**\n\n   - Pros:\n     - Centralized logic, no engine routing.\n   - Cons:\n     - Would quickly become unmaintainable.\n     - Hard to reason about language-specific code paths.\n     - Difficult for contributors to work in a huge file.\n\n**Why we chose this**\n\n- We want **maximum reuse** across languages with **clear modularity**.\n- The pipeline matches how many NLG / grammar engineering systems are structured:\n  - semantics ‚Üí constructions ‚Üí morphology/lexicon.\n- It allows **family engines** to share heavy logic, while still giving languages a way to override via data.\n\n---\n\n## 2. Family Engines Instead of Language-Specific Engines\n\n### Decision: ~15 family engines (Romance, Germanic, Slavic, Agglutinative, Bantu, etc.)\n\n**What we do**\n\n- Implement a small number of **family-level engines** in `engines/*.py`.\n- Each engine is responsible for:\n  - common morphosyntactic patterns for that family,\n  - reading a shared family matrix JSON,\n  - applying language-specific overrides via cards.\n\n**Alternative**\n\n- One engine per language (`engines/it.py`, `engines/es.py`, `engines/sw.py`, ‚Ä¶).\n\n**Why we chose this**\n\n- Many languages share core behavior:\n  - Romance: gender, articles, similar suffix morphology.\n  - Slavic: case system, gendered past, similar declensional logic.\n  - Bantu: noun class + concord across the clause.\n- Factoring this at the family level:\n  - **reduces code duplication**,\n  - makes it easier to add new languages in that family,\n  - encourages us to think in **typological terms** (which is how AW often reasons).\n- Language-specific irregularities are encoded in:\n  - JSON cards,\n  - lexicon entries,\n  - small overrides, not separate engines.\n\n---\n\n## 3. Constructions as a Separate Layer\n\n### Decision: `constructions/*.py` are language-agnostic sentence patterns\n\n**What we do**\n\n- Constructions know about:\n  - argument structure (subject, object, possessor, etc.),\n  - information structure (topic/focus),\n  - clause-level word order and connectors (copula, complementizers, etc.).\n- They do **not** know how to inflect; they call the engine‚Äôs Morphology API.\n\n**Alternative**\n\n- Hard-code constructions inside each engine (or even each language).\n\n**Why we chose this**\n\n- Many constructions are cross-linguistic:\n  - ‚ÄúX is a Y‚Äù, ‚ÄúX has Y‚Äù, ‚ÄúThere is Y in X‚Äù, relative clauses, topic-comment.\n- Having an explicit constructions layer:\n  - Allows us to **reuse constructions** across families.\n  - Makes the system easier to reason about for linguists (they can see ‚Äúthe set of constructions‚Äù).\n  - Separates sentence patterns from morphophonology and low-level syntax.\n- It also lines up well with:\n  - **construction grammar** and **frame semantics** ideas,\n  - where constructions are first-class objects.\n\n---\n\n## 4. Data-Driven Morphology and Configuration\n\n### Decision: Morphology and configuration live in JSON matrices + cards\n\n**What we do**\n\n- Store morphosyntactic rules in JSON:\n\n  - Family matrices in `data/morphology_configs/*.json`.\n  - Per-language cards in `data/<family>/<lang>.json`.\n\n- Engines read these and never hard-code suffixes, article maps, noun classes, etc., unless strictly necessary.\n\n**Alternative**\n\n- Store all morphology directly in Python code (if/else trees, hard-coded tables).\n\n**Why we chose this**\n\n- JSON is:\n  - editable by non-programmers,\n  - easier to version and diff for rules,\n  - suitable for Wikifunctions / Z-data style representations.\n- It enables the **‚Äúcrowdsource the cards‚Äù** strategy:\n  - A contributor can improve `ca.json` or `sw.json` without writing Python.\n- It provides a clear path for:\n  - exporting / importing configs to and from AW / Wikidata objects,\n  - building UI tools for editing language behavior.\n\n---\n\n## 5. Separate Lexicon Subsystem\n\n### Decision: Lexicon is its own package (`lexicon/*`) and data (`data/lexicon/*.json`)\n\n**What we do**\n\n- Lexicon functionality is not hidden in engines.\n- We have:\n  - `lexicon/types.py` ‚Äì Lexeme and related structures,\n  - `lexicon/loader.py`, `lexicon/index.py` ‚Äì loading + indexing,\n  - `lexicon/wikidata_bridge.py`, `lexicon/aw_lexeme_bridge.py` ‚Äì integration with Wikidata/AW lexemes,\n  - `data/lexicon/*.json` ‚Äì actual lexicon files per language.\n\n**Alternatives**\n\n1. Lexicon integrated into engines (per-language dictionaries inside Python).\n2. Use only Wikidata at runtime with no local lexicon.\n\n**Why we chose this**\n\n- Separating lexicon:\n  - makes it reusable by **multiple engines and constructions**,\n  - allows **large-scale** lexicon management (coverage reports, schemas),\n  - makes integration with Wikidata lexemes explicit and testable.\n- Local lexica:\n  - avoid being strictly dependent on live network calls to Wikidata,\n  - allow us to guarantee availability and performance,\n  - can be built / refreshed offline from dumps.\n\n---\n\n## 6. Semantics and Discourse: Light but Real\n\n### Decision: Minimal but explicit semantics (`semantics/*`) and discourse (`discourse/*`)\n\n**What we do**\n\n- Use small dataclasses like `BioFrame`, `Entity`, `Event`, `TimeSpan` for input.\n- Maintain a `DiscourseState` for:\n  - tracking mentioned entities,\n  - salience and last mention,\n  - topic selection.\n- Provide modules for:\n  - information structure (`discourse/info_structure.py`),\n  - referring expressions (`discourse/referring_expression.py`),\n  - simple discourse planning (`discourse/planner.py`).\n\n**Alternative**\n\n- Treat each sentence independently; no discourse model.\n- Encode semantics as ad-hoc dicts with no consistent structure.\n\n**Why we chose this**\n\n- Multi-sentence texts (biography leads, short descriptions) **need** some discourse model:\n  - pronouns vs full names,\n  - topic markers vs canonical word order,\n  - ordering of information.\n- A fully formal semantic/discourse system (UMR, full AMR, etc.) would be heavy for the initial implementation.\n- This ‚Äúlight but real‚Äù layer:\n  - is enough to demonstrate **non-trivial discourse competence**,\n  - stays simple enough for contributors to understand,\n  - provides a bridge to more formal semantic inputs from AW.\n\n---\n\n## 7. Test-First and QA-Focused Design\n\n### Decision: QA tools and test suites are first-class parts of the architecture\n\n**What we do**\n\n- `qa/test_runner.py` and `qa_tools/universal_test_runner.py` are the default way to validate engines.\n- `qa_tools/test_suite_generator.py` helps create large CSV-based suites (often with LLM assistance).\n- `qa_tools/lexicon_coverage_report.py` measures lexicon coverage against tests.\n- Dedicated tests for lexicon loading and Wikidata bridges.\n\n**Alternative**\n\n- Rely primarily on manual testing and ad-hoc scripts.\n- Only add tests at the very end, language by language.\n\n**Why we chose this**\n\n- Abstract Wikipedia needs **trustworthy** renderers:\n  - changes can affect many languages at once.\n- A structured QA pipeline:\n  - catches regressions when modifying family matrices,\n  - quantifies coverage and quality per language,\n  - makes it easier to onboard new languages with confidence.\n- CSV-based suites are:\n  - easy to inspect,\n  - easy to generate semi-automatically (e.g. via LLM prompts),\n  - easy to extend by the community.\n\n---\n\n## 8. Implementation Language and Style\n\n### Decision: Plain Python + JSON, with modest typing\n\n**What we do**\n\n- Use Python for:\n  - engines, constructions, semantics, discourse, lexicon logic.\n- Use JSON for:\n  - morphology configs,\n  - language cards,\n  - lexicon files,\n  - some datasets.\n- Use simple type hints and dataclasses where helpful.\n\n**Alternatives**\n\n- A fully typed/compiled language (e.g. Haskell, OCaml, Rust).\n- A DSL / custom language for grammars.\n\n**Why we chose this**\n\n- Python is:\n  - accessible to many contributors,\n  - already used in Wikimedia / AW-related tooling,\n  - easy to prototype in and profile.\n- JSON is:\n  - compatible with Wikifunctions / Z-data style,\n  - familiar to Wikimedia/MediaWiki ecosystem,\n  - easy to manipulate from many languages and tools.\n\n---\n\n## 9. Scope and Non-Goals\n\n### Explicit non-goals (for now)\n\n- **Full coverage of all sentence types** in every language.\n- **Complete semantic theory** or full alignment with any one formalism (UMR, AMR, Ninai).\n- **Parsing**; the system is generation-focused.\n- **End-to-end styling / register control**, beyond what basic discourse choices allow.\n\nThe design aims to:\n\n- Solve the **architecture problem** for multilingual NLG in AW,\n- Provide a serious, extensible base that can be:\n  - critiqued,\n  - improved,\n  - specialized for particular language families.\n\n---\n\n## 10. Summary\n\nThe key design choices are:\n\n- **Family engines** instead of per-language engines.\n- A separate, **language-agnostic constructions layer**.\n- **Data-driven morphology** via family matrices and language cards.\n- A **distinct lexicon subsystem** integrated with Wikidata.\n- A **light but explicit semantics + discourse layer**.\n- A **test-first, QA-heavy** workflow.\n\nTogether, these choices are intended to make Abstract Wiki Architect:\n\n- scalable across many languages,\n- understandable by both engineers and linguists,\n- and robust enough for real-world Abstract Wikipedia use.\n"
    },
    {
      "rel_path": "internal/ENGINE_INTERNALS.md",
      "ext": ".md",
      "size_bytes": 5898,
      "kind": "markdown",
      "content": "Here is the formal documentation for the **Abstract Wiki Architect: Multilingual Engine V2**.\n\nThis document defines the architecture, directory structure, and logic required to scale the system from \\~40 to 300+ languages using a **Hybrid Factory** approach.\n\n-----\n\n# üìò Project Abstract: The Hybrid Multilingual Engine\n\n### 1\\. Core Philosophy\n\nTo support the 300+ languages required by Abstract Wikipedia, we cannot rely solely on the academic **Resource Grammar Library (RGL)**, which covers only \\~40 languages. Conversely, manual implementation of 260+ languages is unscalable.\n\nWe adopt a **Three-Tier Hybrid Architecture**:\n\n1.  **Prioritize Quality:** Use official, expert-written grammars where available.\n2.  **Allow Overrides:** Enable manual community contributions for specific languages.\n3.  **Guarantee Coverage:** Programmatically generate \"Pidgin\" (simplified) grammars for all remaining languages to ensure 100% API availability.\n\n### 2\\. The \"Waterfall\" Lookup Logic\n\nThe build system (`build_300.py`) will resolve a language code (e.g., `zul`) by checking sources in a specific priority order.\n\n  * **Priority 1: Official RGL.** *Is it in the standard library?* If yes, use it.\n  * **Priority 2: Contrib.** *Do we have a manual draft in `gf/contrib`?* If yes, use it (overrides RGL).\n  * **Priority 3: Factory.** *Is it in the generated folder?* If yes, use it.\n  * **Fail:** If none exist, skip the language.\n\n-----\n\n### 3\\. File Arborescence (Directory Structure)\n\nThe project structure is reorganized to separate **Source** (Official), **Manual** (Community), and **Generated** (Factory) assets.\n\n```text\nC:\\MyCode\\AbstractWiki\\abstract-wiki-architect\\\n‚îÇ\n‚îú‚îÄ‚îÄ architect_http_api\\\n‚îÇ   ‚îî‚îÄ‚îÄ gf\\\n‚îÇ       ‚îî‚îÄ‚îÄ language_map.py       # [ROUTER] Maps Z-IDs (Z1002) to Concrete Names (WikiEng)\n‚îÇ\n‚îú‚îÄ‚îÄ docker\\\n‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile.backend        # [ENV] Copies all 3 grammar folders into the container\n‚îÇ\n‚îú‚îÄ‚îÄ gf\\\n‚îÇ   ‚îú‚îÄ‚îÄ Wiki.pgf                  # [ARTIFACT] The compiled binary containing ALL languages\n‚îÇ   ‚îú‚îÄ‚îÄ build_300.py              # [BUILDER] Orchestrates the \"Waterfall\" compilation\n‚îÇ   ‚îÇ\n‚îÇ   ‚îú‚îÄ‚îÄ gf-rgl\\                   # [TIER 1] Official RGL (Cloned Git Repo)\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ src\\                  #    Contains: english, french, chinese...\n‚îÇ   ‚îÇ\n‚îÇ   ‚îú‚îÄ‚îÄ contrib\\                  # [TIER 2] Manual Overrides (Version Controlled)\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ que\\                  #    Example: High-quality manual Quechua grammar\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ WikiQue.gf\n‚îÇ   ‚îÇ\n‚îÇ   ‚îî‚îÄ‚îÄ generated\\                # [TIER 3] Factory Output (Git Ignored / Auto-Cleaned)\n‚îÇ       ‚îú‚îÄ‚îÄ zul\\                  #    Example: Auto-generated Zulu stubs\n‚îÇ       ‚îú‚îÄ‚îÄ yor\\                  #    Example: Auto-generated Yoruba stubs\n‚îÇ       ‚îî‚îÄ‚îÄ ... (250+ others)\n‚îÇ\n‚îú‚îÄ‚îÄ utils\\\n‚îÇ   ‚îî‚îÄ‚îÄ grammar_factory.py        # [GENERATOR] Reads config, writes files to 'gf/generated/'\n‚îÇ\n‚îî‚îÄ‚îÄ test_gf_dynamic.py            # [VERIFIER] Dynamically tests every language in Wiki.pgf\n```\n\n-----\n\n### 4\\. Component Definitions\n\n#### A. The Language Factory (`utils/grammar_factory.py`)\n\n  * **Purpose:** To ensure no language is left behind.\n  * **Input:** A configuration dictionary defining the \"DNA\" of missing languages (Name, ISO Code, Word Order).\n  * **Output:** Valid, compilable GF source files (`Res`, `Syntax`, `Wiki`) implementing a simplified \"Pidgin\" grammar (e.g., SVO string concatenation).\n  * **Lifecycle:** Runs *before* the build. Wipes and recreates the `gf/generated/` folder every time.\n\n#### B. The Orchestrator (`gf/build_300.py`)\n\n  * **Purpose:** To bind all separate grammar files into a single Portable Grammar Format (`.pgf`) file.\n  * **Logic:**\n      * Iterates through a master list of 300+ ISO codes.\n      * Resolves the file path for each code using the **Waterfall Logic**.\n      * Injects the `gf/generated` and `gf/contrib` paths into the compiler's search scope.\n      * Generates the bridging `Wiki{Lang}.gf` file for RGL languages to connect them to our API.\n\n#### C. The Router (`language_map.py`)\n\n  * **Purpose:** To translate external identifiers into internal GF concrete grammar names.\n  * **Logic:**\n      * `get_concrete(\"fra\")` -\\> `WikiFre` (Legacy RGL naming)\n      * `get_concrete(\"zul\")` -\\> `WikiZul` (Standard Factory naming)\n      * `get_concrete(\"Z1002\")` -\\> `WikiEng` (Wikidata ID mapping)\n\n-----\n\n### 5\\. Developer Workflows\n\n#### Scenario A: Adding a \"Missing\" Language\n\n  * **Goal:** Add *Hausa* (`hau`), which is not in the RGL.\n  * **Action:** Open `utils/grammar_factory.py` and add to the config:\n    ```python\n    \"hau\": {\"name\": \"Hausa\", \"order\": \"SVO\"}\n    ```\n  * **Result:** The next build automatically creates `WikiHau` and compiles it.\n\n#### Scenario B: \"Graduating\" a Language\n\n  * **Goal:** Replace the \"Pidgin\" Zulu grammar with a high-quality manual one.\n  * **Action:**\n    1.  Create folder `gf/contrib/zul/`.\n    2.  Write (or paste) the high-quality `WikiZul.gf` files there.\n  * **Result:** `build_300.py` detects the folder in `contrib`, ignores the one in `generated`, and compiles the high-quality version.\n\n#### Scenario C: Updating the Core\n\n  * **Goal:** Get the latest fixes for English or French.\n  * **Action:** Run `git pull` inside `gf/gf-rgl/`.\n\n-----\n\n### 6\\. Implementation Checklist\n\n1.  ‚úÖ **Architecture Defined.**\n2.  ‚¨ú **Create Factory Script:** `utils/grammar_factory.py`.\n3.  ‚¨ú **Update Orchestrator:** `gf/build_300.py` (Waterfall logic).\n4.  ‚¨ú **Update Router:** `language_map.py` (300+ code support).\n5.  ‚¨ú **Update Docker:** Copy `contrib` and `generated` folders.\n6.  ‚¨ú **Verification:** Run `test_gf_dynamic.py`.\n\n"
    },
    {
      "rel_path": "internal/REFERENCE_LINGUISTICS.md",
      "ext": ".md",
      "size_bytes": 11108,
      "kind": "markdown",
      "content": "# THEORY_NOTES.md  \nAbstract Wiki Architect ‚Äì Theoretical Positioning\n\nThis document explains how the architecture in this repository relates to\nexisting ideas in NLG and linguistic theory. It is not an implementation\nspec; it is a map of **where the design is coming from** and **what it is\ncompatible with**.\n\n---\n\n## 1. Purpose\n\nAbstract Wiki Architect is designed as a **practical NLG stack** for Abstract Wikipedia, but its internal structure is informed by:\n\n- **Grammar engineering / grammar matrix approaches**\n- **Construction grammar / frame semantics**\n- **Abstract semantic formalisms** (UMR, Ninai-style representations)\n- **Typological ‚Äúlanguage family‚Äù thinking**\n\nThe goal is to be:\n\n- **Engineered enough** for large-scale deployment, and  \n- **Theory-aware enough** that it can cooperate with research-grade tools and ideas.\n\n---\n\n## 2. High-level analogy: where this sits\n\nVery roughly:\n\n- Like **Grammatical Framework (GF)**:\n  - We distinguish *family-level* grammar logic and *language-specific* data.\n  - We use shared, language-agnostic constructions (patterns) plus per-language realizations.\n\n- Like a **Grammar Matrix** project:\n  - We factor out cross-language patterns into configurable ‚Äúmatrices‚Äù and ‚Äúcards‚Äù, instead of writing each language from scratch.\n  - We have family templates that are parameterized by language data.\n\n- Like **frame semantics / construction grammar**:\n  - We model recurrent *constructions* (e.g. ‚ÄúX is a Y‚Äù, ‚ÄúX has Y‚Äù, relative clauses) as reusable blocks.\n  - Constructions are not tied to any single language; they are parameterized by an engine and a language profile.\n\n- Like **UMR / Ninai / abstract semantic notations**:\n  - We accept frame-like semantic inputs (`BioFrame` etc.).\n  - We assume we can map richer AW formalisms onto these frames later.\n\nThis project does **not** try to re-implement those formalisms directly; it borrows their *separation of concerns* and *modular structure*.\n\n---\n\n## 3. Relation to specific ideas / traditions\n\n### 3.1 Grammatical Framework (GF)\n\nGF separates:\n\n- **Abstract syntax** (language-independent structures, e.g. `Predication subj pred`)\n- **Concrete syntaxes** (per-language mappings to surface strings)\n\nIn this project:\n\n- `semantics/types.py` + `constructions/*.py` play the role of **abstract syntax and linearization rules**:\n  - Frames and roles (AGENT, PATIENT, TOPIC, etc.)\n  - Constructions that map frames to clause-level structures.\n\n- `engines/*.py` + `data/morphology_configs/*.json` + `data/<family>/<lang>.json` play the role of **concrete syntax/morphology**:\n  - Word order,\n  - Morphological realization,\n  - Agreement patterns.\n\nKey difference:\n\n- We do **not** define a full GF-style type system.\n- We aim for a *lighter-weight, JSON-driven* approach that fits Wikifunctions constraints and is easier to maintain by non-experts.\n\n### 3.2 Grammar Matrix / broad-coverage grammars\n\nGrammar matrix projects (e.g. HPSG-based) build:\n\n- A **cross-linguistic core**, and\n- Language-specific ‚Äúslugs‚Äù or configurations that plug into it.\n\nIn this project:\n\n- The ‚Äúmatrix‚Äù idea appears as:\n\n  - Family matrices in `data/morphology_configs/*.json`\n  - Language cards in `data/<family>/<lang>.json`\n  - Shared constructions in `constructions/*.py`\n\n- Each family engine is, conceptually, a **parameterized grammar sketch**:\n  - It knows what kinds of inflection and syntax the family has (cases, noun classes, harmony, etc.),\n  - It pulls concrete parameters from JSON.\n\nWe do not implement a full constraint-based grammar formalism, but we:\n\n- Encapsulate **morphosyntactic parameters** in structured data,\n- Make it possible to **derive many languages from a single family template**.\n\n### 3.3 Construction Grammar and Frame Semantics\n\nThe constructions modules are explicitly constructional:\n\n- Each file in `constructions/` describes a **reusable mapping** from roles and features to syntactic structure, e.g.:\n\n  - Predicate nominals, locatives, existentials,\n  - Possession with ‚Äúhave‚Äù vs ‚Äúbe + genitive‚Äù,\n  - Relative clauses (subject gap, object gap),\n  - Topic‚Äìcomment structures.\n\n- They operate on **semantic role labels** (subject, possessor, theme, etc.) and **information structure labels** (topic/focus).\n\nThis is closely aligned with:\n\n- **Construction grammar**:\n  - Constructions are the primary units, not just rules about individual words.\n- **Frame semantics**:\n  - Certain constructions correspond to lexical or grammatical frames (e.g. `Being_born`, `Possession`, `Residence`).\n\nWe keep the representation pragmatic:\n\n- Roles are simple strings,\n- Frames are Python dataclasses (e.g. `BioFrame`),\n- But the design is open to being mapped to richer framebanks or AW‚Äôs own abstract representations.\n\n### 3.4 Abstract semantic formalisms (UMR, Ninai, etc.)\n\nAbstract Wikipedia has explored notations such as:\n\n- **Ninai** (a compact notation for abstract content),\n- Mappings from structured Wikidata statements to abstract meaning representations,\n- Other AMR-like or UMR-like representations.\n\nIn this project:\n\n- `semantics/types.py` defines *implementation-oriented frames* for now (e.g. biography-centric).\n- `semantics/aw_bridge.py` / `semantics/normalization.py` are intended to be the place where:\n\n  - AW‚Äôs chosen notation (Ninai, UMR, etc.) is mapped into internal frame types.\n  - We lose as little information as possible, but we adapt to what constructions/engines expect.\n\nThe philosophy is:\n\n- Keep the **NLG architecture** stable,\n- Allow **input formalisms** to change or evolve,\n- Provide clear mapping points rather than baking in any specific notation forever.\n\n---\n\n## 4. Internal abstractions and why they look like this\n\n### 4.1 Family Engines\n\nAssumption:\n\n- A large number of languages share **core structural properties**:\n\n  - Agglutinative vs fusional vs isolating,\n  - Case vs classifier vs noun class,\n  - Topic‚Äìcomment vs subject‚Äìpredicate orientation.\n\nFamily engines:\n\n- Capture these recurring patterns once (e.g. ‚Äúagglutinative + harmony + suffix chaining‚Äù),\n- Parameterize the specifics via JSON and lexicon data.\n\nThis is a typological, not purely genetic, grouping:\n\n- ‚ÄúAgglutinative‚Äù includes Turkic, Uralic, Dravidian, Koreanic features,\n- ‚ÄúBantu‚Äù groups noun class systems with concord,\n- ‚ÄúIsolating‚Äù covers Chinese-like analytic grammars.\n\nThe design is influenced by:\n\n- WALS-style typology (word order, morph type),\n- Practical ‚Äúlanguage family‚Äù divisions that linguists and engineers already use.\n\n### 4.2 Constructions vs Engines\n\nReason for splitting:\n\n- Engines know **how** a language inflects and orders elements.\n- Constructions know **what configuration** of elements a clause needs.\n\nThis separation:\n\n- Allows the same construction code to be reused across families,\n- Keeps engines agnostic about specific predicate types (e.g. biographies vs locations vs events),\n- Encourages **compositionality**: semantics ‚Üí construction ‚Üí morphology.\n\n### 4.3 Semantics and Discourse\n\nWe deliberately use:\n\n- Simple dataclasses (`BioFrame`, `Event`, `Entity`, `TimeSpan`) instead of a full logical language.\n- A **minimal DiscourseState** with:\n\n  - salience,\n  - last-mention position,\n  - topic tracking.\n\nThe idea is:\n\n- Capture just enough information to make **real discourse decisions**:\n  - pronoun vs full NP,\n  - topic markers vs canonical word order,\n  - sentence ordering for short texts.\n\nThis keeps the system usable for production while leaving room to:\n\n- Align with richer discourse and anaphora theories later,\n- Plug in more elaborate centering or information-structure models if needed.\n\n---\n\n## 5. Design tradeoffs\n\n### 5.1 Expressiveness vs maintainability\n\nWe deliberately do **not**:\n\n- Implement full-blown HPSG or LFG-style grammars,\n- Implement a full type system √† la GF abstract syntax,\n- Enforce a single deep semantic formalism.\n\nInstead, we choose:\n\n- JSON-encoded parameters and lexica,\n- Python constructions and engines that can be read by non-specialists,\n- A design that is understandable by both linguists and software engineers.\n\n### 5.2 Family-based generalization vs per-language precision\n\n- Using family engines risks **over-generalization**: some languages are typological hybrids or have idiosyncrasies.\n- The architecture counters this by:\n  - Allowing language cards to override or extend family patterns,\n  - Using the lexicon to encode idiosyncratic behavior,\n  - Letting constructions ask for language-specific flags where needed.\n\n### 5.3 NLG-first vs parsing-first\n\nThis project is **NLG-first**:\n\n- All modules are oriented toward generation, not parsing.\n- But the separation into semantics, constructions, morphology, and lexicon means:\n  - A future parsing side could re-use portions of the same data,\n  - Or at least be developed in parallel with consistent categories.\n\n---\n\n## 6. Future theoretical directions\n\nThis architecture is intended to be a **bridge** between practical AW deployments and more research-oriented work. Natural extensions include:\n\n1. **Richer frame inventory**  \n   - Generalize `BioFrame` into a set of core frames:\n     - Birth, death, office holding, discovery, award, location, membership, etc.\n   - Align those with AW‚Äôs semantic notations and Wikidata schema.\n\n2. **Closer integration with UMR / Ninai**  \n   - Define explicit mappings from UMR/Ninai structures to internal frames.\n   - Track information-structure annotations in those notations.\n\n3. **More powerful discourse models**  \n   - Add centering-based or game-theoretic models for anaphora and topic shifts.\n   - Support longer multi-sentence texts while preserving coherence.\n\n4. **Learned components on top of rule-based scaffolding**  \n   - Keep rules as the **backbone** for grammar,\n   - Explore learned models for:\n     - lexical choice within a frame,\n     - micro-variation in word order,\n     - style control.\n\n5. **Closer alignment with GF / grammar engineering tools**  \n   - Export parts of the matrices/cards as GF lexica or vice versa.\n   - Treat the family engines as ‚ÄúGF light‚Äù where appropriate.\n\n---\n\n## 7. Summary\n\n- The architecture is **inspired by** GF, Grammar Matrix projects, construction grammar, and frame semantics, but implemented in a pragmatic, JSON-driven, Python-based form suitable for Abstract Wikipedia and Wikifunctions.\n- It aims to:\n  - Separate **semantics** from **constructions** from **morphosyntax** from **lexicon**,\n  - Share as much logic as possible across **language families**,\n  - Allow both linguists and engineers to collaborate on a shared code/data base.\n\nThese notes are here to make explicit that the system is not ‚Äújust a pile of scripts‚Äù, but a conscious engineering interpretation of long-standing ideas in formal and computational linguistics.\n"
    },
    {
      "rel_path": "internal/RESEARCH_GF_MAPPING.md",
      "ext": ".md",
      "size_bytes": 17024,
      "kind": "markdown",
      "content": "\n# GF Integration Design\n\nStatus: draft  \nOwner: (TBD)  \nScope: integrate selected parts of **Grammatical Framework (GF)** into **Abstract Wiki Architect (AWA)** without changing AWA‚Äôs core architecture.\n\n---\n\n## 1. Goals and Non-Goals\n\n### 1.1 Goals\n\n1. **Leverage GF‚Äôs linguistic work** without adopting GF as the core platform:\n   - Reuse GF‚Äôs **morphology paradigms** (RGL) to strengthen AWA‚Äôs morphology configs.\n   - Borrow **syntactic patterns** and **test cases** to improve constructions and QA.\n\n2. **Keep AWA‚Äôs architecture intact**:\n   - Still use **family engines + JSON configs + lexicon subsystem + frames**.\n   - No dependency on GF in the runtime generation pipeline.\n\n3. **Make integration repeatable and auditable**:\n   - Clear tooling to re-import/refresh GF-derived resources.\n   - All GF-derived data carries provenance metadata (version, module, language).\n\n4. **Stay license-clean and modular**:\n   - GF is BSD-style; we must preserve attribution.\n   - GF integration must be optional and clearly separated in the repo.\n\n### 1.2 Non-Goals\n\n- Do **not**:\n  - Port GF‚Äôs compiler or AST model into AWA.\n  - Make AWA dependent on GF for runtime generation.\n  - Implement parsing or reversible grammars.\n  - Port full GF concrete syntaxes per language.\n\n- We only use GF as:\n  - an **offline source of paradigms, rule patterns, and test cases**, and\n  - conceptual inspiration for constructions.\n\n---\n\n## 2. Scope Overview\n\nThe integration is split into three phases (loosely independent):\n\n1. **Phase 1 ‚Äì Morphology integration (high priority)**  \n   Toolchain to import GF‚Äôs noun/adj/verb paradigms into AWA‚Äôs JSON morphology configs and/or language cards.\n\n2. **Phase 2 ‚Äì Syntax pattern harvesting (medium priority)**  \n   Human-guided extraction of word order and construction patterns from RGL; ported into AWA constructions and family engines.\n\n3. **Phase 3 ‚Äì Test case integration (medium priority)**  \n   Use GF grammars to generate contrastive test pairs for morphology and syntax; import them as CSV rows into AWA‚Äôs QA suites.\n\n---\n\n## 3. Current AWA Architecture (Short Recap)\n\nCore elements relevant for integration:\n\n- `engines/` ‚Äì **family engines** orchestrating:\n  - morphology (`morphology/<family>.py`),\n  - constructions (`constructions/...`),\n  - lexicon (`lexicon/...`).\n\n- `data/morphology_configs/` ‚Äì **family grammar matrices**:\n  - e.g. `romance_grammar_matrix.json`, `slavic_matrix.json`.\n  - Capture family-level paradigms, morphological category space, default rules.\n\n- `data/<family>/<lang>.json` ‚Äì **language cards**:\n  - overrides and language-specific morphological parameters.\n\n- `data/lexicon/*.json` ‚Äì lexica:\n  - `lemma`, `pos`, features (gender, number, etc.), IDs.\n\n- QA pipeline:\n  - `qa_tools/test_suite_generator.py` ‚Üí CSV templates.\n  - `qa/test_runner.py` ‚Üí AWA generation vs expected outputs.\n  - `qa_tools/lexicon_coverage_report.py` ‚Üí lexicon coverage.\n\nGF integration will feed into:\n\n- `data/morphology_configs/*`  \n- `data/<family>/*`  \n- `qa_tools/generated_datasets/*` (test CSVs)\n\nNothing in the runtime core (router, engines, constructions) should require GF at runtime.\n\n---\n\n## 4. GF Overview (Only What We Use)\n\n- GF provides:\n  - **Resource Grammar Library (RGL)**:\n    - per-language morphology and syntax modules,\n    - lexical categories, inflection paradigms, function signatures.\n\n- Relevant pieces for us:\n  1. **Morphology paradigms** (inflection tables, rules).\n  2. **Syntactic constructors** (clause building, phrase building).\n  3. **Regression tests** and examples used to validate RGL.\n\nWe will treat GF as an **offline tool** installed locally (or in CI) for generation of intermediate artifacts.\n\n---\n\n## 5. High-Level Integration Architecture\n\n### 5.1 Components\n\n1. **GF Export Layer** (external dependency, optional)\n   - Small GF scripts or commands to dump:\n     - paradigm tables (lemma ‚Üí forms),\n     - sample sentences for constructions.\n\n2. **Conversion Layer (`gf_integration/`)** (new in AWA)\n   - Python utilities to transform GF exports into:\n     - AWA morphology JSON (grammar matrix + language cards),\n     - AWA QA CSV rows,\n     - optional helper JSON/YAML for syntactic patterns.\n\n3. **Consumption Layer** (existing AWA code)\n   - AWA family engines and morphology modules continue to read:\n     - `data/morphology_configs/*`,\n     - `data/<family>/*.json`,\n     - `qa_tools/generated_datasets/*.csv`.\n\n### 5.2 Data Flow (Morphology)\n\n```text\nGF RGL .gf files\n     ‚îÇ\n     ‚îú‚îÄ[gf export scripts]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  gf_morph_export.json  (intermediate)\n     ‚îÇ\n     ‚îî‚îÄ[gf_integration/convert_morphology.py]‚ñ∫\n             data/morphology_configs/<family>_gf_enriched.json\n             data/<family>/<lang>_gf_card.json\n````\n\n### 5.3 Data Flow (QA Tests)\n\n```text\nGF grammars + test ASTs\n     ‚îÇ\n     ‚îú‚îÄ[gf export tests]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  gf_test_sentences.csv\n     ‚îÇ\n     ‚îî‚îÄ[gf_integration/convert_tests.py]‚îÄ‚ñ∫\n             qa_tools/generated_datasets/test_suite_<lang>_gf.csv\n```\n\n### 5.4 Isolation\n\n* All GF-dependent scripts live under:\n\n  * `gf_integration/` and/or `tools/gf/`.\n* Generated artifacts are:\n\n  * committed to `data/*` and `qa_tools/generated_datasets/*` as plain JSON/CSV.\n* Runtime never calls GF; it only reads JSON/CSV.\n\n---\n\n## 6. Phase 1 ‚Äì Morphology Integration\n\n### 6.1 Target: morphology configs and language cards\n\nWe want to:\n\n* enrich `data/morphology_configs/<family>_grammar_matrix.json` with:\n\n  * more complete paradigms,\n  * better irregular rules,\n  * more accurate stem/affix patterns;\n* optionally create/extend `data/<family>/<lang>.json` from GF data.\n\n### 6.2 Source: GF morphology\n\nFor each language `L`:\n\n* GF RGL module typically has:\n\n  * categories like `N`, `A`, `V`, `Adv`, `Det`,\n  * inflection functions like:\n\n    * `mkN` (noun paradigm),\n    * `mkA` (adjective paradigm),\n    * `mkV` (verb paradigm),\n    * plus irregular entries.\n\n* We treat these as describing:\n\n  * paradigm type,\n  * inflection slots (e.g. `Mas Sg`, `Fem Pl`, person/tense endings),\n  * phonological adjustments.\n\n### 6.3 Intermediate model\n\nDefine an intermediate JSON schema, e.g. `gf_morph_export.json`:\n\n```json\n{\n  \"language\": \"it\",\n  \"gf_version\": \"3.12\",\n  \"paradigms\": [\n    {\n      \"category\": \"N\",\n      \"name\": \"Noun_o_a\",\n      \"slots\": [\"Masc_Sg\", \"Fem_Sg\", \"Masc_Pl\", \"Fem_Pl\"],\n      \"examples\": [\n        {\n          \"lemma\": \"amico\",\n          \"forms\": {\n            \"Masc_Sg\": \"amico\",\n            \"Fem_Sg\": \"amica\",\n            \"Masc_Pl\": \"amici\",\n            \"Fem_Pl\": \"amiche\"\n          }\n        }\n      ],\n      \"rules\": {\n        \"stem_pattern\": \".*o\",\n        \"transformations\": [\n          { \"from\": \"o\", \"to\": \"a\", \"slot\": \"Fem_Sg\" },\n          { \"from\": \"o\", \"to\": \"i\", \"slot\": \"Masc_Pl\" },\n          { \"from\": \"co\", \"to\": \"che\", \"slot\": \"Fem_Pl\", \"condition\": \"...\"}\n        ]\n      }\n    }\n  ]\n}\n```\n\nThe exact structure can be adapted, but we need:\n\n* category,\n* slot inventory,\n* transformation rules,\n* at least one exemplar per paradigm.\n\nThis file is produced by GF export scripts (outside AWA Python).\n\n### 6.4 Conversion to AWA grammar matrices\n\nCreate `gf_integration/convert_morphology.py`:\n\nResponsibilities:\n\n1. **Read intermediate export** (`gf_morph_export.json`).\n2. **Map GF categories and slots to AWA categories and features**:\n\n   * e.g. `category=\"N\"` ‚Üí AWA `pos=\"NOUN\"`.\n   * `slot=\"Masc_Sg\"` ‚Üí `{\"gender\": \"masc\", \"number\": \"sg\"}`.\n3. **Inject/merge into AWA family matrix**:\n\n   * open `data/morphology_configs/romance_grammar_matrix.json`,\n   * update or extend:\n\n     * paradigm definitions,\n     * suffix rules,\n     * phonological rules.\n4. **Emit enriched config files**:\n\n   * either overwrite the existing matrix, or\n   * generate `*_gf_enriched.json` and then manually promote.\n\n#### 6.4.1 Mapping table (sketch)\n\nCreate a static mapping in `gf_integration/mappings.py`:\n\n```python\nGF_POS_TO_AWA = {\n    \"N\": \"NOUN\",\n    \"A\": \"ADJ\",\n    \"V\": \"VERB\",\n    \"Adv\": \"ADV\",\n    # ...\n}\n\nGF_SLOT_TO_FEATURES = {\n    \"Masc_Sg\": {\"gender\": \"masc\", \"number\": \"sg\"},\n    \"Fem_Sg\":  {\"gender\": \"fem\",  \"number\": \"sg\"},\n    \"Masc_Pl\": {\"gender\": \"masc\", \"number\": \"pl\"},\n    \"Fem_Pl\":  {\"gender\": \"fem\",  \"number\": \"pl\"},\n    # ...\n}\n```\n\nThese mappings are family-specific and may be extended as we add more languages.\n\n### 6.5 Language cards\n\nWhere GF provides language-specific quirks that should not live in the shared family matrix (e.g. special elision, clitic binding, orthographic niceties), we record them in:\n\n* `data/<family>/<lang>_gf_card.json` or merge into existing `data/<family>/<lang>.json`.\n\nExample fields:\n\n```json\n{\n  \"language\": \"it\",\n  \"source\": \"gf-rgl-3.12\",\n  \"article_elision\": [\n    { \"before_vowel\": true, \"article\": \"lo\", \"elided\": \"l'\" }\n  ],\n  \"special_clusters\": [\n    { \"pattern\": \"s + consonant\", \"article\": \"uno\" }\n  ]\n}\n```\n\n### 6.6 Integration with existing morphology code\n\nAfter generating the enriched configs:\n\n1. Update `morphology/<family>.py` to:\n\n   * recognise new paradigm IDs,\n   * apply the newly imported transformations.\n\n2. Add regression tests in `qa/test_*.py`:\n\n   * for languages where GF data is integrated,\n   * verifying a few paradigms end-to-end.\n\n3. Update `docs/LEXICON_ARCHITECTURE.md` and `docs/LEXICON_WORKFLOW.md`:\n\n   * note that some paradigms are GF-derived,\n   * specify refresh steps.\n\n### 6.7 Versioning and provenance\n\nEvery GF-derived file must include provenance in its `meta`:\n\n```json\n{\n  \"_meta\": {\n    \"source\": \"gf-rgl\",\n    \"gf_version\": \"3.12\",\n    \"gf_modules\": [\"Italian/Noun.gf\", \"Italian/Adjective.gf\"],\n    \"generated_at\": \"2025-12-06T12:00:00Z\"\n  },\n  \"paradigms\": [ ... ]\n}\n```\n\n---\n\n## 7. Phase 2 ‚Äì Syntax Pattern Harvesting\n\nThis phase is more manual and conceptual than Phase 1.\n\n### 7.1 Objective\n\nUse GF‚Äôs syntactic design to refine AWA‚Äôs:\n\n* constructions (word order, phrase structure),\n* family engine logic (clitics, argument order, topicalisation).\n\nWe **do not** import GF syntax code directly. Instead, we:\n\n* inspect GF modules (RGL `Syntax`, `Paradigms`, `Sentence`),\n* translate key patterns into AWA constructions.\n\n### 7.2 Workflow\n\n1. **Identify focus constructions** in AWA:\n\n   * simple copula (`copula_equative_simple`, `copula_equative_classification`),\n   * basic clauses (`intransitive_event`, `transitive_event`),\n   * relative clauses (`relative_clause_subject_gap`),\n   * coordination, comparatives, existentials.\n\n2. For each construction:\n\n   * review corresponding GF RGL functions (e.g. `mkCl`, `mkS`, `mkRS`, etc.) for several languages,\n   * document word-order rules and parameterization:\n\n     * SVO vs SOV,\n     * adjective position (pre/post),\n     * negation particles, clitics.\n\n3. Amend AWA constructions:\n\n   * add configuration hooks in `constructions/base.py` for:\n\n     * clause skeleton templates,\n     * slot reordering (subject, verb, object, adverbials),\n     * optional positions for topic markers or clitics.\n   * set defaults per family/language in `language_profiles/profiles.json` and/or family matrices.\n\n4. Add examples to docs:\n\n   * for each major pattern, include a brief note in `docs/FRAMES_NARRATIVE.md` or separate `docs/SYNTAX_NOTES.md`.\n\n### 7.3 Deliverables\n\n* `docs/GF_SYNTAX_NOTES_<family>.md`:\n\n  * summary of imported GF insights per language family.\n* Adjusted constructions and engines:\n\n  * more accurate word order,\n  * more consistent cross-language parameterisation.\n\n---\n\n## 8. Phase 3 ‚Äì Test Case Integration\n\n### 8.1 Objective\n\nUse GF‚Äôs grammars as a **test oracle** to generate:\n\n* contrastive examples for morphology (correct vs wrong forms),\n* syntactic minimal pairs (e.g. clitic placement, agreement).\n\nThese become rows in AWA‚Äôs CSV test suites.\n\n### 8.2 Data Flow\n\n```text\nGF test grammars / examples\n     ‚îÇ\n     ‚îú‚îÄ[gf generate tests]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  gf_test_sentences.csv\n     ‚îÇ      (lang, phenomenon, input parameters, output_strings[])\n     ‚îÇ\n     ‚îî‚îÄ[gf_integration/convert_tests.py]‚ñ∫\n             qa_tools/generated_datasets/test_suite_<lang>_gf.csv\n```\n\nExample intermediate CSV:\n\n```csv\nlang,phenomenon,input,good,bad1,bad2\nit,adj_agreement,\"Adj=grande,N=ragazzo\", \"il grande ragazzo\",\"il grande ragazza\",\"i grande ragazzo\"\n```\n\n`convert_tests.py` then:\n\n* maps each row to one or more AWA test rows:\n\n  * build the corresponding `BioFrame` or other frame,\n  * place `good` as `EXPECTED_OUTPUT`,\n  * optionally use `badX` as negative examples or comments.\n\n### 8.3 Integration with QA\n\n* `qa_tools/test_suite_generator.py`:\n\n  * extended to optionally include GF-derived tests when generating new CSVs.\n* `qa/test_runner.py`:\n\n  * treat GF-derived test suites the same as human-authored ones.\n* Optionally, dedicated GF-specific suites:\n\n  * `test_suite_<lang>_gf_morph.csv`,\n  * `test_suite_<lang>_gf_syntax.csv`.\n\nProvenance columns:\n\n* `SOURCE=gf_rgl`,\n* `GF_VERSION`,\n* `GF_MODULE`.\n\n---\n\n## 9. Repo Layout Changes\n\nProposed additions:\n\n```text\nabstract-wiki-architect/\n  gf_integration/\n    __init__.py\n    mappings.py\n    convert_morphology.py\n    convert_tests.py\n    README.md           # how to install GF, run exporters, etc.\n\n  tools/gf/             # optional\n    export_morphology.gf\n    export_tests.gf\n\n  docs/\n    GF_INTEGRATION.md   # this document\n    GF_SYNTAX_NOTES_romance.md\n    GF_SYNTAX_NOTES_slavic.md\n    ...\n```\n\n---\n\n## 10. Implementation Plan\n\n### Milestone 1 ‚Äì Skeleton and one language (Romance: Italian)\n\n1. Add `gf_integration/` skeleton and mappings.\n2. Write minimal GF exporter:\n\n   * `tools/gf/export_morphology.gf` for Italian.\n3. Generate `gf_morph_export_it.json`.\n4. Implement `convert_morphology.py` ‚Üí update `romance_grammar_matrix.json` and `data/romance/it.json`.\n5. Extend `morphology/romance.py` to use new paradigms.\n6. Add a small GF-based test suite for Italian morphology.\n7. Verify tests; update docs.\n\n### Milestone 2 ‚Äì Additional languages in same family\n\n1. Repeat for Spanish, French, Portuguese, Romanian.\n2. Consolidate mappings for Romance.\n3. Add shared `GF_SYNTAX_NOTES_romance.md` and adjust constructions as needed.\n\n### Milestone 3 ‚Äì Another family (Slavic, Germanic, etc.)\n\nRepeat Milestone 1‚Äì2 pattern per family.\n\n### Milestone 4 ‚Äì Test integration\n\n1. Implement `export_tests.gf` for at least one language.\n2. Generate test CSV and integrate via `convert_tests.py`.\n3. Wire into QA generator/runner.\n4. Document process in `docs/GF_INTEGRATION.md`.\n\n---\n\n## 11. Risks and Mitigations\n\n### 11.1 Risk: mismatch between GF and AWA feature sets\n\n* GF may have richer morphological categories or slightly different category splits.\n* Mitigation:\n\n  * design explicit mappings in `gf_integration/mappings.py`,\n  * allow partial import (ignore unused categories),\n  * add validation to flag unmapped slots.\n\n### 11.2 Risk: overfitting to GF assumptions\n\n* GF grammars reflect specific theoretical choices that may not align with AW needs.\n* Mitigation:\n\n  * GF is used as **one source**, not authoritative truth,\n  * manual review of paradigm and pattern imports per language.\n\n### 11.3 Risk: maintenance burden\n\n* GF may evolve; re-importing paradigms might break configs.\n* Mitigation:\n\n  * capture GF version in meta,\n  * treat imports as semi-manual: run scripts, inspect diffs, then commit,\n  * do not auto-refresh GF imports in CI without human review.\n\n---\n\n## 12. Licensing and Attribution\n\n* GF is BSD-licensed; include:\n\n  * a pointer to GF‚Äôs LICENSE in `docs/GF_INTEGRATION.md`,\n  * explicit note in generated files‚Äô `_meta.source` fields.\n\nExample meta snippet:\n\n```json\n{\n  \"_meta\": {\n    \"source\": \"GF Resource Grammar Library\",\n    \"license\": \"BSD-style\",\n    \"url\": \"https://www.grammaticalframework.org/\",\n    \"gf_version\": \"3.12\"\n  }\n}\n```\n\n---\n\n## 13. Summary\n\n* We **do not** adopt GF‚Äôs architecture.\n\n* We **do** import:\n\n  * morphology paradigms,\n  * syntactic insights,\n  * test cases.\n\n* Runtime remains:\n\n  * frames ‚Üí discourse ‚Üí constructions ‚Üí family engines ‚Üí AWA morphology + lexicon.\n\nGF becomes an **offline knowledge provider** for richer paradigms and tests, under clear versioning and provenance.\n\nThis design keeps Abstract Wiki Architect aligned with Abstract Wikipedia/Wikifunctions needs while leveraging decades of grammar-engineering work in GF.\n\n"
    }
  ]
}