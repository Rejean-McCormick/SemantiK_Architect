from __future__ import annotations

from typing import Dict, Sequence

from .config import DEFAULT_TIMEOUT_SEC, PYTHON_EXE
from .models import ToolSpec


def py_script(
    tool_id: str,
    rel_script: str,
    description: str,
    *,
    timeout_sec: int = DEFAULT_TIMEOUT_SEC,
    allow_args: bool = False,
    allowed_flags: Sequence[str] = (),
    allow_positionals: bool = False,
    requires_ai_enabled: bool = False,
    flags_with_value: Sequence[str] = (),
    flags_with_multi_value: Sequence[str] = (),
) -> ToolSpec:
    return ToolSpec(
        tool_id=tool_id,
        description=description,
        rel_target=rel_script,
        cmd=(PYTHON_EXE, "-u", "{target}"),
        timeout_sec=timeout_sec,
        allow_args=allow_args,
        allowed_flags=tuple(allowed_flags),
        allow_positionals=allow_positionals,
        requires_ai_enabled=requires_ai_enabled,
        flags_with_value=tuple(flags_with_value),
        flags_with_multi_value=tuple(flags_with_multi_value),
    )


def pytest_file(
    tool_id: str,
    rel_test_file: str,
    description: str,
    *,
    timeout_sec: int = DEFAULT_TIMEOUT_SEC,
    allow_args: bool = False,
    allowed_flags: Sequence[str] = (),
    flags_with_value: Sequence[str] = (),
) -> ToolSpec:
    return ToolSpec(
        tool_id=tool_id,
        description=description,
        rel_target=rel_test_file,
        cmd=(PYTHON_EXE, "-u", "-m", "pytest", "{target}"),
        timeout_sec=timeout_sec,
        allow_args=allow_args,
        allowed_flags=tuple(allowed_flags),
        allow_positionals=False,
        requires_ai_enabled=False,
        flags_with_value=tuple(flags_with_value),
        flags_with_multi_value=(),
    )


def qa_registry() -> Dict[str, ToolSpec]:
    return {
        # --- QA & TESTING ---
        "ambiguity_detector": py_script(
            "ambiguity_detector",
            "tools/qa/ambiguity_detector.py",
            "Uses AI to generate ambiguous sentences and checks for multiple parse trees.",
            timeout_sec=1200,
            allow_args=True,
            allowed_flags=("--lang", "--sentence", "--topic", "--json-out", "--verbose"),
            allow_positionals=False,
            requires_ai_enabled=True,
            flags_with_value=("--lang", "--sentence", "--topic", "--json-out"),
        ),
        "run_smoke_tests": pytest_file(
            "run_smoke_tests",
            "tests/test_lexicon_smoke.py",
            "Lexicon schema/syntax smoke tests.",
            timeout_sec=900,
            allow_args=True,
            allowed_flags=("-q", "-vv", "-k", "-m", "--maxfail", "--disable-warnings"),
            flags_with_value=("-k", "-m", "--maxfail"),
        ),
        "run_judge": pytest_file(
            "run_judge",
            "tests/integration/test_quality.py",
            "Executes Golden Standard regression checks (AI Judge integration).",
            timeout_sec=1800,
            allow_args=True,
            allowed_flags=("-q", "-vv", "-k", "-m", "--maxfail", "--disable-warnings"),
            flags_with_value=("-k", "-m", "--maxfail"),
        ),
        "eval_bios": py_script(
            "eval_bios",
            "tools/qa/eval_bios.py",
            "Compares generated biographies against Wikidata facts.",
            timeout_sec=1200,
            allow_args=True,
            allowed_flags=("--langs", "--limit", "--out", "--verbose"),
            allow_positionals=False,
            flags_with_value=("--limit", "--out"),
            flags_with_multi_value=("--langs",),
        ),
        "lexicon_coverage": py_script(
            "lexicon_coverage",
            "tools/qa/lexicon_coverage_report.py",
            "Coverage report for intended vs implemented lexicon.",
            timeout_sec=1200,
            allow_args=True,
            allowed_flags=("--langs", "--out", "--format", "--verbose", "--fail-on-errors"),
            allow_positionals=False,
            flags_with_value=("--out", "--format"),
            flags_with_multi_value=("--langs",),
        ),

        # UPDATED: universal_test_runner flags now match tools/qa/universal_test_runner.py
        "universal_test_runner": py_script(
            "universal_test_runner",
            "tools/qa/universal_test_runner.py",
            "Advanced CSV test runner (supports more complex constructions).",
            timeout_sec=1800,
            allow_args=True,
            allowed_flags=(
                "--dataset-dir",
                "--pattern",
                "--langs",
                "--limit",
                "--fail-fast",
                "--strict",
                "--print-failures",
                "--json-report",
                "--verbose",
            ),
            allow_positionals=False,
            flags_with_value=(
                "--dataset-dir",
                "--pattern",
                "--limit",
                "--print-failures",
                "--json-report",
            ),
            flags_with_multi_value=("--langs",),
        ),
        "test_runner": py_script(
            "test_runner",
            "tools/qa/universal_test_runner.py",
            "Legacy alias for universal_test_runner.",
            timeout_sec=1800,
            allow_args=True,
            allowed_flags=(
                "--dataset-dir",
                "--pattern",
                "--langs",
                "--limit",
                "--fail-fast",
                "--strict",
                "--print-failures",
                "--json-report",
                "--verbose",
            ),
            allow_positionals=False,
            flags_with_value=(
                "--dataset-dir",
                "--pattern",
                "--limit",
                "--print-failures",
                "--json-report",
            ),
            flags_with_multi_value=("--langs",),
        ),

        "batch_test_generator": py_script(
            "batch_test_generator",
            "tools/qa/batch_test_generator.py",
            "Bulk generation of test datasets for regression.",
            timeout_sec=1800,
            allow_args=True,
            allowed_flags=("--langs", "--out", "--limit", "--seed", "--verbose"),
            allow_positionals=False,
            flags_with_value=("--out", "--limit", "--seed"),
            flags_with_multi_value=("--langs",),
        ),
        "test_suite_generator": py_script(
            "test_suite_generator",
            "tools/qa/test_suite_generator.py",
            "Generates empty CSV templates for manual/AI fill-in.",
            timeout_sec=900,
            allow_args=True,
            allowed_flags=("--langs", "--out", "--verbose"),
            allow_positionals=False,
            flags_with_value=("--out",),
            flags_with_multi_value=("--langs",),
        ),
        "generate_lexicon_regression_tests": py_script(
            "generate_lexicon_regression_tests",
            "tools/qa/generate_lexicon_regression_tests.py",
            "Builds regression tests from lexicon for CI/QA.",
            timeout_sec=1800,
            allow_args=True,
            allowed_flags=("--langs", "--out", "--limit", "--verbose", "--lexicon-dir"),
            allow_positionals=False,
            flags_with_value=("--out", "--limit", "--lexicon-dir"),
            flags_with_multi_value=("--langs",),
        ),

        # --- AI SERVICES (gated) ---
        "seed_lexicon": py_script(
            "seed_lexicon",
            "utils/seed_lexicon_ai.py",
            "Uses LLM to generate core vocabulary for new languages.",
            timeout_sec=3600,
            allow_args=True,
            allowed_flags=("--langs", "--limit", "--dry-run", "--verbose"),
            allow_positionals=False,
            requires_ai_enabled=True,
            flags_with_value=("--limit",),
            flags_with_multi_value=("--langs",),
        ),
        "ai_refiner": py_script(
            "ai_refiner",
            "tools/ai_refiner.py",
            "Attempts to upgrade Pidgin grammars to full RGL.",
            timeout_sec=3600,
            allow_args=True,
            allowed_flags=("--langs", "--dry-run", "--verbose"),
            allow_positionals=True,
            requires_ai_enabled=True,
            flags_with_multi_value=("--langs",),
        ),

        # --- TESTS (Inventory) ---
        "test_api_smoke": pytest_file(
            "test_api_smoke",
            "tests/test_api_smoke.py",
            "API smoke tests (fast signal).",
            timeout_sec=600,
            allow_args=True,
            allowed_flags=("-q", "-vv", "-k", "-m", "--maxfail", "--disable-warnings"),
            flags_with_value=("-k", "-m", "--maxfail"),
        ),
        "test_gf_dynamic": pytest_file(
            "test_gf_dynamic",
            "tests/test_gf_dynamic.py",
            "Validates dynamic loading/linearization of GF grammars.",
            timeout_sec=900,
            allow_args=True,
            allowed_flags=("-q", "-vv", "-k", "-m", "--maxfail", "--disable-warnings"),
            flags_with_value=("-k", "-m", "--maxfail"),
        ),
        "test_multilingual_generation": pytest_file(
            "test_multilingual_generation",
            "tests/test_multilingual_generation.py",
            "Multilingual generation regression tests.",
            timeout_sec=1200,
            allow_args=True,
            allowed_flags=("-q", "-vv", "-k", "-m", "--maxfail", "--disable-warnings"),
            flags_with_value=("-k", "-m", "--maxfail"),
        ),
    }