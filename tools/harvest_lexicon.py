import argparse
import json
import re
import requests
import sys
from pathlib import Path

# --- CONFIGURATION: GF WORDNET ---
# Regex for Root WordNet.gf
RE_ABSTRACT = re.compile(r"fun\s+([^\s:]+).*?--\s*([Q\d]+-?[a-z0-9]*)")
# Regex for Root WordNetEng.gf
RE_CONCRETE = re.compile(r"lin\s+(\w+)\s*=\s*(.*?)\s*;")
RE_STRING = re.compile(r'"([^"]+)"')

# --- CONFIGURATION: WIKIDATA ---
SPARQL_TEMPLATE = """
SELECT ?item ?itemLabel ?itemDescription WHERE {
  VALUES ?item { %s }
  SERVICE wikibase:label { bd:serviceParam wikibase:language "%s". }
}
"""

class GFWordNetHarvester:
    def __init__(self, root_path):
        self.root = Path(root_path)
        self.semantic_map = {} 

    def load_semantics(self):
        """Step 1: Index the Abstract Keys (Directly from Root)"""
        # CORRECTED: Look in root, not root/gf
        path = self.root / "WordNet.gf"
        
        if not path.exists():
            print(f"‚ùå Critical: WordNet.gf not found at {path}")
            sys.exit(1)

        print(f"üìñ Indexing semantics from {path}...")
        count = 0
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                match = RE_ABSTRACT.search(line)
                if match:
                    func, sem_id = match.groups()
                    self.semantic_map[func.strip()] = sem_id.strip()
                    count += 1
        
        print(f"   Indexed {count} semantic keys.")

    def harvest_lang(self, lang_code, out_dir):
        """Step 2: Extract words from Concrete Grammar"""
        # CORRECTED: Look in root, not root/gf
        fname = f"WordNet{lang_code.capitalize()}.gf"
        src_file = self.root / fname

        if not src_file.exists():
            print(f"‚ö†Ô∏è  Skipping {lang_code}: {fname} not found in {self.root}")
            return

        print(f"üöú Harvesting {lang_code} from {src_file.name}...")
        lexicon = {}
        count = 0

        with open(src_file, 'r', encoding='utf-8') as f:
            content = f.read()

        for match in RE_CONCRETE.finditer(content):
            func, rhs = match.groups()
            
            if "variants {}" in rhs: continue

            strings = RE_STRING.findall(rhs)
            if strings:
                lemma = strings[0]
                sem_id = self.semantic_map.get(func, "")
                
                entry = {
                    "lemma": lemma,
                    "gf_fun": func,
                    "source": "gf-wordnet"
                }
                
                if sem_id.startswith("Q"):
                    entry["qid"] = sem_id
                elif sem_id:
                    entry["wnid"] = sem_id

                lexicon[lemma.lower()] = entry
                count += 1

        out_path = Path(out_dir) / lang_code / "wide.json"
        out_path.parent.mkdir(parents=True, exist_ok=True)
        with open(out_path, 'w', encoding='utf-8') as f:
            json.dump(lexicon, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Saved {count} words to {out_path}")

class WikidataHarvester:
    def fetch(self, qids, lang_code):
        print(f"‚òÅÔ∏è  Fetching {len(qids)} labels from Wikidata for '{lang_code}'...")
        values = " ".join([f"wd:{qid}" for qid in qids])
        query = SPARQL_TEMPLATE % (values, lang_code)
        
        try:
            r = requests.get(
                "https://query.wikidata.org/sparql", 
                params={'format': 'json', 'query': query}, 
                timeout=30
            )
            data = r.json()
            return {
                item['item']['value'].split('/')[-1]: {
                    "lemma": item.get('itemLabel', {}).get('value'),
                    "desc": item.get('itemDescription', {}).get('value', "")
                }
                for item in data['results']['bindings']
            }
        except Exception as e:
            print(f"‚ùå Wikidata Error: {e}")
            return {}

def main():
    parser = argparse.ArgumentParser(description="Universal Lexicon Harvester")
    subparsers = parser.add_subparsers(dest="source", required=True)

    wn_parser = subparsers.add_parser("wordnet", help="Mine local GF files")
    wn_parser.add_argument("--root", required=True)
    wn_parser.add_argument("--langs", default="eng")
    wn_parser.add_argument("--out", default="data/lexicon")

    wd_parser = subparsers.add_parser("wikidata", help="Fetch from Cloud")
    wd_parser.add_argument("--lang", required=True)
    wd_parser.add_argument("--input", required=True)
    wd_parser.add_argument("--domain", default="people")

    args = parser.parse_args()

    if args.source == "wordnet":
        harvester = GFWordNetHarvester(args.root)
        harvester.load_semantics()
        for lang in args.langs.split(","):
            harvester.harvest_lang(lang.strip(), args.out)

    elif args.source == "wikidata":
        if not Path(args.input).exists():
             print(f"‚ùå Input file not found: {args.input}")
             sys.exit(1)
        
        with open(args.input, 'r') as f:
            target_qids = json.load(f).keys()
        
        harvester = WikidataHarvester()
        data = harvester.fetch(target_qids, args.lang)
        
        out_path = Path(f"data/lexicon/{args.lang}/{args.domain}.json")
        out_path.parent.mkdir(parents=True, exist_ok=True)
        with open(out_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ Saved to {out_path}")

if __name__ == "__main__":
    main()